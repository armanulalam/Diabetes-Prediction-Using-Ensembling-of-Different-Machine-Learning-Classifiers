{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KrtXQ1F7ry2"
      },
      "source": [
        "### Mount the drive with the colab notebook to access the content in the drive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsV1oCURZYu_",
        "outputId": "9025fb76-a9cc-4e46-d54f-fc69747a27bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/MRA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6R9GNvwZcTl",
        "outputId": "17e29121-9a50-4b0c-c8e4-50cb21c05866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/MRA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'diabetes.csv'"
      ],
      "metadata": {
        "id": "zMpsYNqLZmAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dujqAogI8bQZ"
      },
      "source": [
        "### Loading of different packagaes and APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz80SgiJtd5o",
        "outputId": "fc975f07-dc85-4c7b-a376-10b8e04a02ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.4.1)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(6)\n",
        "import random\n",
        "random.seed(6)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(6)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.svm import SVC, NuSVC\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors  import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy import *\n",
        "from numpy import interp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "!pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Activation, Dense, Dropout, BatchNormalization, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d272noMSoezZ"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5ce3ePzKLDP"
      },
      "outputs": [],
      "source": [
        "Renamed_feature= []               #list of names that will rename to feature column\n",
        "all_clf_res=[]                    #every classifier auc values are stored in it\n",
        "random_initializer=20            #random initializer\n",
        "n_dots=50\n",
        "\n",
        "for i in range(8):\n",
        "  #for renaming dataset of columns features F1 -- F8\n",
        "  Renamed_feature.append('F'+str(i+1))\n",
        "\n",
        "# Pairs plots are just showing all variables paired with all the other variables\n",
        "def pair_plot(data):\n",
        "  '''\n",
        "  This function will create a grid of Axes such that each variable\n",
        "  in data will by shared in the y-axis across a single row and in the x-axis\n",
        "  across a single column.The diagonal Axes are treated differently, drawing\n",
        "  a plot to show the univariate distribution of the data for the variable in\n",
        "  that column.\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable for\n",
        "  plotting pair plot of features in this\n",
        "  dataframe\n",
        "\n",
        "  Output :\n",
        "  This function Plot pairwise relationships in a dataset.\n",
        "\n",
        "  '''\n",
        "  plt.figure()\n",
        "\n",
        "  pair_plot =sns.pairplot(data=data,\n",
        "                          height=3,\n",
        "                          hue='Outcome',\n",
        "                          diag_kind='kde')\n",
        "  # fig.suptitle(\"Pairplot of all features\")\n",
        "  pair_plot.fig.suptitle(\"Pairplot of all features\")\n",
        "  plt.show()\n",
        "\n",
        "# this function for Gaussian distribution plot\n",
        "# and box plot simultaneously in a figure\n",
        "def Box_Gaussian(data):\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Output - The Gaussian distribution plot for\n",
        "  eight feature of input data\n",
        "\n",
        "  '''\n",
        "\n",
        "  fig=plt.figure(figsize=(15,6))                                #define figure size\n",
        "  fig.suptitle(\"Box Gaussian plot of all features\")\n",
        "\n",
        "  n_scaler = preprocessing.StandardScaler()                 #standardization function\n",
        "  temp_Data = n_scaler.fit_transform(data)                  #pass into function for standrd.\n",
        "  for i in range(8):                                        #loop for all 8 feature\n",
        "\n",
        "    plt.subplot(2, 4, i+1)                                  #subplot for 2 rows in 4 columns\n",
        "    Data = temp_Data[:,i]                                   #data for every feature\n",
        "    sns.kdeplot(Data, shade=True,color='red', alpha=0.3)    #kernel density function under red shaded arae\n",
        "    ax = sns.boxplot(Data, saturation=0.9, color=\"green\")   #boxplot  with green shaded area\n",
        "                                                            # https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n",
        "                                                            # https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
        "    plt.gca().invert_yaxis()                                #Reverse Y-Axis in PyPlot\n",
        "    # plt.title('F'+str(i+1))\n",
        "    frame1 = plt.gca()\n",
        "    frame1.axes.xaxis.set_ticklabels([])                    #removing xlabel data\n",
        "    plt.ylim((-0.5,0.65))                                   #y axis  limit\n",
        "    plt.tight_layout()                                      #This module provides routines to adjust subplot params so that subplots are nicely fit in the figure.\n",
        "                                                            # https://matplotlib.org/api/tight_layout_api.html\n",
        "    # plt.grid('on')\n",
        "\n",
        "    for patch in ax.artists:\n",
        "      r, g, b, a = patch.get_facecolor()                     #Get the facecolor of the Axes.\n",
        "      patch.set_facecolor((r, g, b, 0.3))                    #set colour intensity\n",
        "\n",
        "\n",
        "def plot_confusionMatrix(data):\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Output -visualization of correalation matrix of\n",
        "  input data\n",
        "\n",
        "  '''\n",
        "  sns.set(font_scale=1.15)                                    # Set aesthetic parameters in one step.\n",
        "  ax = plt.figure(figsize=(10, 8))                            #set figure size   https://seaborn.pydata.org/generated/seaborn.set.html\n",
        "  plt.title(\"Confusion Matrix of all features\")\n",
        "  sns.heatmap(data.corr(),                                    # input correlation matrix  of dataset\n",
        "              vmax=1.0,                                       #Values to anchor the colormap, otherwise they are inferred from\n",
        "                                                              #the data and other keyword arguments.\n",
        "              vmin=0.0,\n",
        "              linewidths=0.01,\n",
        "              square=False,                                   #If True, set the Axes aspect to “equal” so each cell will be square-shaped.\n",
        "              annot=True,                                     #If True, write the data value in each cell.\n",
        "              linecolor=\"black\")                              #Color of the lines that will divide each cell.\n",
        "                                                              #https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "  b, t = plt.ylim()                                           # discover the values for bottom and top\n",
        "  b += 0.5                                                    # Add 0.5 to the bottom\n",
        "  t -= 0.5                                                    # Subtract 0.5 from the top\n",
        "  plt.ylim(b, t)                                              # update the ylim(bottom, top) values\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# this function plot univariate distribution of every feature\n",
        "\n",
        "def dist_Plot(data):\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Output - The distribution plot for\n",
        "  eight feature of input data\n",
        "\n",
        "  '''\n",
        "  fig, ax = plt.subplots(2,4, figsize=(12,5))                 #set numbers of rows and columns of subplot\n",
        "  sns.set()\n",
        "  sns.distplot(data.F1, bins = 10, ax=ax[0,0])                #Flexibly plot a univariate distribution of observations.\n",
        "  sns.distplot(data.F2, bins = 10, ax=ax[0,1])\n",
        "  sns.distplot(data.F3, bins = 10, ax=ax[0,2])\n",
        "  sns.distplot(data.F4, bins = 10, ax=ax[0,3])\n",
        "  sns.distplot(data.F5, bins = 10, ax=ax[1,0])\n",
        "  sns.distplot(data.F6, bins = 10, ax=ax[1,1])\n",
        "  sns.distplot(data.F7, bins = 10, ax=ax[1,2])\n",
        "  sns.distplot(data.F8, bins = 10, ax=ax[1,3])\n",
        "  fig.suptitle(\"Gaussian Distribution of all features\")\n",
        "  fig.tight_layout()                                          #This module provides routines to adjust subplot params\n",
        "                                                              #  so that subplots are nicely fit in the figure.\n",
        "\n",
        "\n",
        "# this function plot violin plot  of every feature\n",
        "\n",
        "\n",
        "def plot_violinplot (data):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Output - The violinplot plot for\n",
        "  eight feature of input data\n",
        "\n",
        "  '''\n",
        "  fig, ax = plt.subplots(2,4, figsize=(12,6))\n",
        "  # #set numbers of rows and columns of subplot and figure size\n",
        "  sns.set()\n",
        "  sns.violinplot(x = data.Outcome, y=data.F1,  ax=ax[0,0])    #violine plot for F1 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F2,  ax=ax[0,1])    #violine plot for F2 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F3,  ax=ax[0,2])    #violine plot for F3 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F4,  ax=ax[0,3])    #violine plot for F4 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F5,  ax=ax[1,0])    #violine plot for F5 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F6,  ax=ax[1,1])    #violine plot for F6 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F7,  ax=ax[1,2])    #violine plot for F7 feature\n",
        "  sns.violinplot(x = data.Outcome, y=data.F8,  ax=ax[1,3])    #violine plot for F8 feature\n",
        "  fig.suptitle(\"Violin plot of all features\")\n",
        "  fig.tight_layout()\n",
        "\n",
        "                                                              # https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
        "\n",
        "\n",
        "\n",
        "#this function  is for manual outleir rejection\n",
        "def Manual (data):\n",
        "\n",
        "    '''\n",
        "    Parameters :\n",
        "    Input - data is the pandas type variable\n",
        "\n",
        "    Return - dataframe with outleir rejection\n",
        "    of input data\n",
        "\n",
        "    '''\n",
        "    # input dataset is data\n",
        "    max_Pregnancies = data.F1.max()                         #maximum feature of F1\n",
        "    data = data[data.F1!=max_Pregnancies]                   #find  where extreme value is absent and remove extreme\n",
        "    max_Glucose = data.F2.max()                             #maximum feature of F2\n",
        "    data = data[data.F2!=max_Glucose]                       #find  where extreme value is absent and remove\n",
        "    for i in range(4):                                      #in this loop we succesively remove 4 minimum element\n",
        "      min_Glucose = data.F2.min()                           #find minimum\n",
        "      data = data[data.F2!=min_Glucose]                     #reject minimum\n",
        "    max_BloodPressure = data.F3.max()                       #maximum feature of F3\n",
        "    data = data[data.F3!=max_BloodPressure]                 #find  where extreme value is absent and remove\n",
        "    for i in range(2):                                      #in this loop we succesively remove 2 extreme element\n",
        "      max_skinthickness = data.F4.max()\n",
        "      data = data[data.F4!=max_skinthickness]\n",
        "    for i in range(25):                                     #in this loop we succesively remove 25 extreme element\n",
        "      max_Insulin = data.F5.max()\n",
        "      data = data[data.F5!=max_Insulin]\n",
        "    max_bmi = data.F6.max()\n",
        "    data = data[data.F6!=max_bmi]\n",
        "    for i in range(4):                                      #in this loop we succesively remove 4 minimum element\n",
        "      min_bmi = data.F6.min()\n",
        "      data = data[data.F6!=min_bmi]\n",
        "    for i in range(20):                                     #in this loop we succesively remove 20 extreme element\n",
        "      max_DiabetesPedigreeF = data.F7.max()\n",
        "      data = data[data.F7!=max_DiabetesPedigreeF]\n",
        "    for i in range(20):                                     #in this loop we succesively remove 20 extreme element\n",
        "      max_age = data.F8.max()\n",
        "      data = data[data.F8!=max_age]\n",
        "      df =data\n",
        "    return data\n",
        "\n",
        "\n",
        "# this function if for outlair rejection with\n",
        "# respect to mean value\n",
        "def IQR_Mean (data):\n",
        "\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Return - dataframe with outleir rejection\n",
        "  filled with mean\n",
        "  of input data\n",
        "\n",
        "  '''\n",
        "  for i in range(8):\n",
        "    x = data[Renamed_feature[i]]\n",
        "    Q1 = x.quantile(0.25)                                   # Q1 is the \"middle\" value in the first half of the rank-ordered data set.\n",
        "    Q3 = x.quantile(0.75)                                   # Q3 is the \"middle\" value in the second half of the rank-ordered data set.\n",
        "    IQR = Q3-Q1                                             # The interquartile range is equal to Q3 minus Q1.\n",
        "    mean = x.mean()                                         #mean of feature\n",
        "    for j in range(569):                                    # loop for first 569 elements of feature\n",
        "      temp = x[j]                                           # every feature value\n",
        "      LW = (Q1 - 1.5 * IQR)                                 #lower considerable range of gaussian distribution\n",
        "      UW = (Q3 + 1.5 * IQR)                                 #upper considerable range of gaussian distribution\n",
        "      if temp < LW:                                         #replace upper value with mean\n",
        "        x[j] = mean\n",
        "      if temp > UW:                                         #replace lower value with mean\n",
        "        x[j] = mean\n",
        "    data[Renamed_feature[i]] = x\n",
        "  return data\n",
        "\n",
        "# this function if for outlair rejection with respect to median value same as previous function\n",
        "def IQR_Median (data):\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Return - dataframe with outleir rejection\n",
        "  filled with median\n",
        "  of input data\n",
        "\n",
        "  '''\n",
        "  for i in range(8):\n",
        "    x = data[Renamed_feature[i]]\n",
        "    Q1 = x.quantile(0.25)\n",
        "    Q3 = x.quantile(0.75)\n",
        "    IQR = Q3-Q1\n",
        "    median = x.quantile(0.5)                                # find the median\n",
        "    for j in range(569):                                    #replace the first 569 values with respect to median\n",
        "      temp = x[j]\n",
        "      LW = (Q1 - 1.5 * IQR)\n",
        "      UW = (Q3 + 1.5 * IQR)\n",
        "      if temp < LW:                                         #replace upper value with median\n",
        "        x[j] = median\n",
        "      if temp > UW:\n",
        "        x[j] = median                                       #replace upper value with median\n",
        "    data[Renamed_feature[i]] = x\n",
        "  return data\n",
        "\n",
        "\n",
        "def IQR (data):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input - data is the pandas type variable\n",
        "\n",
        "  Return - dataframe with outleir rejection\n",
        "  of input data\n",
        "\n",
        "  '''\n",
        "  #input dataset as data\n",
        "  for i in range(8):                                        # for every feature\n",
        "    Q1 = data[Renamed_feature[i]].quantile(0.25)\n",
        "    Q3 = data[Renamed_feature[i]].quantile(0.75)\n",
        "    IQR = Q3-Q1                                             #find IQR\n",
        "    LW = (Q1 - 1.5 * IQR)                                   #find lower boundary\n",
        "          # print(LW)\n",
        "    UW = (Q3 + 1.5 * IQR)                                   #find upper boundary\n",
        "          # print(UW)\n",
        "    data = data[data[Renamed_feature[i]]<UW]                #drop greater than upper limit\n",
        "    data = data[data[Renamed_feature[i]]>LW]                #drop smaller than lower limit\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "#outlier rejection with different condition\n",
        "\n",
        "def outlier_Rejection (data, iqr_Mean, iqr_Medain, iqr, manual):\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  data is the pandas type variable\n",
        "  iqr_Mean - for outleir rejection with Mean\n",
        "  iqr_Medain- for outleir rejection with Medain\n",
        "  iqr- for drop the outleir\n",
        "  manual -for manual rejection\n",
        "  Return - dataframe with outleir rejection\n",
        "  filled with Input parameter\n",
        "\n",
        "  '''\n",
        "\n",
        "  # outlier_Rejection with conditional input\n",
        "  if iqr_Mean == True:                                     #reject outleir with Mean\n",
        "    data = IQR_Mean (data)\n",
        "  if iqr_Medain == True:                                   #reject outleir with Median\n",
        "    data = IQR_Medain (data)\n",
        "  if iqr == True:                                          #reject outleir in IQR range\n",
        "    data = IQR (data)\n",
        "  if manual == True:                                       #reject outleir with manual\n",
        "    data = Manual (data)\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "#data plot on different input condition\n",
        "def data_plot (data,\n",
        "               Pair_plot,\n",
        "               Dist_Plot,\n",
        "               Plot_violinplot,\n",
        "               Plot_confusionMatrix,\n",
        "               box_Gaussian):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  data - It is the pandas type variable\n",
        "  Pair_plot - for pair plot visualization of input  data\n",
        "  Dist_Plot- for gaussian distribution plot visualization of input  data\n",
        "  Plot_violinplot- for violin plot visualization of input  data\n",
        "  Plot_confusionMatrix -for confusion matrix visualization of input  data\n",
        "\n",
        "  Output - dataframe with outleir rejection\n",
        "  filled with Input parameter\n",
        "\n",
        "  '''\n",
        "  if Pair_plot ==True:\n",
        "    pair_plot(data)\n",
        "\n",
        "  if Dist_Plot ==True:\n",
        "    dist_Plot(data)\n",
        "\n",
        "  if Plot_violinplot ==True:\n",
        "    plot_violinplot (data)\n",
        "\n",
        "  if Plot_confusionMatrix ==True:\n",
        "    plot_confusionMatrix(data)\n",
        "\n",
        "  if box_Gaussian ==True:\n",
        "    Box_Gaussian(data)\n",
        "\n",
        "\n",
        "def replace_zero(data, field, target):\n",
        "\n",
        "    mean_by_target = data.loc[data[field] != 0, [field, target]].groupby(target).mean()\n",
        "    data.loc[(data[field] == 0)&(data[target] == 0), field] = mean_by_target.iloc[0][0]\n",
        "    data.loc[(data[field] == 0)&(data[target] == 1), field] = mean_by_target.iloc[1][0]\n",
        "\n",
        "\n",
        "def metrics (y_true, y_pred, probas_):\n",
        "\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  y_true - true  value of input data\n",
        "  y_pred- predicted  value of input data\n",
        "  probas_- probability/confidence of predicted output\n",
        "\n",
        "  return -True Negative(tn),False Positive(fp),False Negative(fn)\n",
        "  True positive(tp),AUC(roc_auc),False Positive Rate(fpr),\n",
        "  True positive rate(tpr)\n",
        "\n",
        "  '''\n",
        "\n",
        "\n",
        "  points=n_dots*'-'\n",
        "  print(points)\n",
        "#    print(\"Best parameters set found on development set:\")\n",
        "#    print(clf.best_params_)\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, probas_[:, 1])\n",
        "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "  tprs[-1][0] = 0.0\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  #  aucs.append(roc_auc)\n",
        "  print(\"Detailed classification report for current fold:\")\n",
        "  print()\n",
        "  print(classification_report(y_true, y_pred))\n",
        "  print()\n",
        "  print(\"Area Under ROC (AUC): {}\".format(roc_auc))\n",
        "  print()\n",
        "  print('Confusion Matrix for current fold: ')\n",
        "  print(confusion_matrix(y_true, y_pred))\n",
        "  print()\n",
        "  print(\"Accuracy for Current Fold: {}\".format(accuracy_score(y_true, y_pred)))\n",
        "  print()\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "  return  tn, fp, fn, tp, roc_auc, fpr, tpr\n",
        "\n",
        "\n",
        "\n",
        "def average_ROC(mean_fpr,tprs,aucs,TP,TN,FP,FN):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  mean_fpr - Mean False positive rate\n",
        "  tprs -values of true positive rate\n",
        "  aucs  - values of auc\n",
        "  TP    - True positive\n",
        "  TN    - True Negative\n",
        "  FP    - False Positiv\n",
        "  FN    - False Negative\n",
        "\n",
        "  Output -\n",
        "  Visalization of TPR vs FPR plot\n",
        "  '''\n",
        "  sen = (np.sum(TP))/(np.sum(TP)+np.sum(FN))\n",
        "  spe = (np.sum(TN))/(np.sum(TN)+np.sum(FP))\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = np.mean(aucs)\n",
        "  std_auc = np.std(aucs)\n",
        "  ax = plt.axes()\n",
        "  ax.grid(color='lightgray', linestyle='-', linewidth=.5)\n",
        "  ax.set_facecolor(\"white\")\n",
        "\n",
        "  ax.spines['bottom'].set_color('#000000')\n",
        "  ax.spines['top'].set_color('#000000')\n",
        "  ax.spines['right'].set_color('#000000')\n",
        "  ax.spines['left'].set_color('#000000')\n",
        "\n",
        "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
        "          label=r'Avg. ROC (AUC (avg $\\pm$ std) = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),\n",
        "          lw=2, alpha=.8)\n",
        "\n",
        "  plt.scatter((1-spe), sen, s=80, c='r', marker='x',)\n",
        "  plt.scatter(0, sen, s=80, c='r', marker='x',)\n",
        "  plt.scatter((1-spe),0, s=80, c='r', marker='x',)\n",
        "  plt.axhline(y=sen, color='r', linestyle='--')\n",
        "  plt.axvline(x=(1-spe), color='r', linestyle='--')\n",
        "  plt.text((1-spe), 0.02, \"FPR={:0.3f}\".format((1-spe)))\n",
        "  plt.text(0.009, sen+0.05, \"TPR={:0.3f}\".format(sen))\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "  plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='darkgray', alpha=0.5,\n",
        "                  label=r'$\\pm$ 1 Standard deviation')\n",
        "\n",
        "  plt.xticks(np.arange(0.0, 1.01, step=0.1))\n",
        "  plt.yticks(np.arange(0.0, 1.01, step=0.1))\n",
        "  left=0.0\n",
        "  right=1.0\n",
        "  plt.xlim(left, right)\n",
        "  plt.ylim(left, right)\n",
        "  plt.xlabel('False Positive Rate (FPR)')\n",
        "  plt.ylabel('True Positive Rate (TPR)')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  # plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def plot_Current_ROC(fpr,tpr,iterator,roc_auc):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  fpr - False positive rate\n",
        "  tpr - True positive rate\n",
        "  roc_auc -auc values of roc curve\n",
        "\n",
        "  Output -\n",
        "  Visalization of current roc curve\n",
        "\n",
        "  '''\n",
        "  plt.plot(fpr,\n",
        "\n",
        "          tpr,\n",
        "          alpha=0.35,\n",
        "          linewidth=1)\n",
        "\n",
        "def creat_Model (classifier, X_Train, Y_Train, tuned_parameters, verbose):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  X_Train -train data\n",
        "  Y_Train - label/output of train data\n",
        "  tuned_parameters =parameters of models\n",
        "  verbose = condition about model\n",
        "\n",
        "  Output -\n",
        "  Returned a tuned classifier using grid search\n",
        "  '''\n",
        "  clf = GridSearchCV(classifier,\n",
        "                    tuned_parameters,\n",
        "                    verbose=verbose,\n",
        "                    cv=5,\n",
        "                    scoring='roc_auc',\n",
        "                    n_jobs=-1)\n",
        "  clf.fit(X_Train, Y_Train)\n",
        "  return clf\n",
        "\n",
        "\n",
        "def average_performance(aucs,Accuracy,TP,TN,FP,FN):\n",
        "\n",
        "  '''\n",
        "  Parameters :\n",
        "  Input -\n",
        "  aucs= values of aucs\n",
        "  Accuracy - value of accuracy\n",
        "  TP  - True Positive\n",
        "  TN  - True Negative\n",
        "  FP  - False Positive\n",
        "  FN  - False Negative\n",
        "\n",
        "\n",
        "  Output -\n",
        "  It prints the average aucs,accuarcy,confusion matrix\n",
        "  '''\n",
        "\n",
        "  print()\n",
        "  n_dotsav=(n_dots-len('Average'))//2\n",
        "\n",
        "  print('-'*n_dotsav+'Average'+'-'*n_dotsav)\n",
        "  print(\"AUC (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(aucs),np.std(aucs)))\n",
        "  print(\"Accuracy (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(Accuracy),np.std(Accuracy)))\n",
        "  cm = [[int(np.mean(TP)), int(np.mean(FP))],[int(np.mean(FN)), int(np.mean(TN))]]\n",
        "  print ('Avg. CM is '+str(cm))\n",
        "  cm = [[int(np.sum(TP)), int(np.sum(FP))],[int(np.sum(FN)), int(np.sum(TN))]]\n",
        "  print ('Total for all folds CM is '+str(cm))\n",
        "  re_auc=str(round(np.mean(aucs), 3))+'+/-'+str(round(np.std(aucs),3))\n",
        "  all_clf_res.append(re_auc)\n",
        "\n",
        "#this  function is for algorithm based feature selection\n",
        "def feature_Selector(data, algo, n_feature):\n",
        "    '''\n",
        "    Parameters :\n",
        "    Input -\n",
        "    data - It is the pandas type variable\n",
        "    algo - type of algorith PCA,ICA,Correlation\n",
        "\n",
        "\n",
        "    Output -\n",
        "    It prints the average aucs,accuarcy,confusion matrix\n",
        "    '''\n",
        "    if algo=='PCA':                                                   #for pca algorithm\n",
        "        X_Data= data.iloc[:,:8].values\n",
        "        pca = PCA(n_components=n_feature)                             #number of feature\n",
        "        X_Data = pca.fit_transform(X_Data)\n",
        "        return X_Data , data.iloc[:,8:].values\n",
        "\n",
        "    if algo == 'ICA':\n",
        "        X_Data= data.iloc[:,:8].values\n",
        "        ICA = FastICA(n_components=n_feature, random_state=12)\n",
        "        X_Data = ICA.fit_transform(X_Data)\n",
        "        return X_Data , data.iloc[:,8:].values\n",
        "\n",
        "    if algo =='corr':                                                   #for ica algorithm\n",
        "        if n_feature ==4:\n",
        "            data = data[['F2','F5','F4','F6','Outcome']]                #for 4 feature\n",
        "            return data.iloc[:,:4].values, data.iloc[:,4:].values\n",
        "        if n_feature ==6:\n",
        "            data = data[['F1','F2','F4','F5','F6','F8','Outcome']]       #for 6 feature\n",
        "            return data.iloc[:,:6].values, data.iloc[:,6:].values\n",
        "\n",
        "    if algo == 'None':\n",
        "        return data.iloc[:,:8].values, data.iloc[:,8:].values            #if feature selection is off all features are counted\n",
        "\n",
        "\n",
        "def run (hLayer,\n",
        "         batch_size = [32],\n",
        "         epochs = [100],\n",
        "         learn_rate = [0.001],\n",
        "         dropout_rate = [0.3],\n",
        "         activation = ['relu'],\n",
        "         init =['normal']):\n",
        "    \"\"\"\n",
        "    Input : hidden layer number\n",
        "    and others are alwalys constant\n",
        "\n",
        "    Output : optimized model with best number of neuron\n",
        "    in every layer\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if hLayer==1:\n",
        "\n",
        "\n",
        "        #  number of neurons  for every layer\n",
        "        #  optimization\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "\n",
        "\n",
        "\n",
        "        # parameter dictionary for grid search#\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2)\n",
        "\n",
        "        # model building function for given parameters\n",
        "        def nn_opt_1(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2):\n",
        "            \"\"\"\n",
        "            Input : activation ,dropout_rate,init,learn_rate,neuron1,neuron2\n",
        "\n",
        "            Output : Using input hyper_parameters build model and return it\n",
        "            \"\"\"\n",
        "\n",
        "            #model initialization and building block\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "            return model\n",
        "\n",
        "        #grid search for optimisaion\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_1, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "\n",
        "        #build model with optimized parameters\n",
        "        model =nn_opt_1(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'])\n",
        "\n",
        "        #  return best parameters and model\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "\n",
        "    if hLayer==2:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3)\n",
        "\n",
        "        def nn_opt_2(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_2, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_2(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==3:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4)\n",
        "\n",
        "        def nn_opt_3(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_3, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_3(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==4:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5)\n",
        "\n",
        "        def nn_opt_4(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_4, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_4(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==5:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6)\n",
        "\n",
        "        def nn_opt_5(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_5, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_5(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "\n",
        "    if hLayer==6:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        neuron7 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6,\n",
        "                          neuron7=neuron7)\n",
        "\n",
        "        def nn_opt_6(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6,\n",
        "                   neuron7):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_6, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_6(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'],\n",
        "                grid_results.best_params_['neuron7'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==7:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        neuron7 = [16, 32, 64]\n",
        "        neuron8 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6,\n",
        "                          neuron7=neuron7,\n",
        "                          neuron8=neuron8)\n",
        "\n",
        "        def nn_opt_7(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6,\n",
        "                   neuron7,\n",
        "                   neuron8):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_7, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_7(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'],\n",
        "                grid_results.best_params_['neuron7'],\n",
        "                grid_results.best_params_['neuron8'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==8:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        neuron7 = [16, 32, 64]\n",
        "        neuron8 = [16, 32, 64]\n",
        "        neuron9 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6,\n",
        "                          neuron7=neuron7,\n",
        "                          neuron8=neuron8,\n",
        "                          neuron9 =neuron9)\n",
        "\n",
        "        def nn_opt_8(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6,\n",
        "                   neuron7,\n",
        "                   neuron8,\n",
        "                   neuron9):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_8, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_8(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'],\n",
        "                grid_results.best_params_['neuron7'],\n",
        "                grid_results.best_params_['neuron8'],\n",
        "                grid_results.best_params_['neuron9'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==9:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        neuron7 = [16, 32, 64]\n",
        "        neuron8 = [16, 32, 64]\n",
        "        neuron9 = [16, 32, 64]\n",
        "        neuron10 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6,\n",
        "                          neuron7=neuron7,\n",
        "                          neuron8=neuron8,\n",
        "                          neuron9=neuron9,\n",
        "                          neuron10=neuron10)\n",
        "\n",
        "        def nn_opt_9(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6,\n",
        "                   neuron7,\n",
        "                   neuron8,\n",
        "                   neuron9,\n",
        "                   neuron10 ):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron10, input_dim = neuron9, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_9, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_9(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'],\n",
        "                grid_results.best_params_['neuron7'],\n",
        "                grid_results.best_params_['neuron8'],\n",
        "                grid_results.best_params_['neuron9'],\n",
        "                grid_results.best_params_['neuron10'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "    if hLayer==10:\n",
        "        neuron1 = [16, 32, 64]\n",
        "        neuron2 = [16, 32, 64]\n",
        "        neuron3 = [16, 32, 64]\n",
        "        neuron4 = [16, 32, 64]\n",
        "        neuron5 = [16, 32, 64]\n",
        "        neuron6 = [16, 32, 64]\n",
        "        neuron7 = [16, 32, 64]\n",
        "        neuron8 = [16, 32, 64]\n",
        "        neuron9 = [16, 32, 64]\n",
        "        neuron10 = [16, 32, 64]\n",
        "        neuron11 = [16, 32, 64]\n",
        "        param_grid = dict(batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          learn_rate=learn_rate,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          activation=activation,\n",
        "                          init=init,\n",
        "                          neuron1=neuron1,\n",
        "                          neuron2=neuron2,\n",
        "                          neuron3=neuron3,\n",
        "                          neuron4=neuron4,\n",
        "                          neuron5=neuron5,\n",
        "                          neuron6=neuron6,\n",
        "                          neuron7=neuron7,\n",
        "                          neuron8=neuron8,\n",
        "                          neuron9 =neuron9,\n",
        "                          neuron10=neuron10,\n",
        "                          neuron11 =neuron11)\n",
        "\n",
        "        def nn_opt_10(activation,\n",
        "                   dropout_rate,\n",
        "                   init,\n",
        "                   learn_rate,\n",
        "                   neuron1,\n",
        "                   neuron2,\n",
        "                   neuron3,\n",
        "                   neuron4,\n",
        "                   neuron5,\n",
        "                   neuron6,\n",
        "                   neuron7,\n",
        "                   neuron8,\n",
        "                   neuron9,\n",
        "                   neuron10,\n",
        "                   neuron11):\n",
        "            model = Sequential()\n",
        "            model.add(Dense(neuron1, input_dim = 6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron6, input_dim = neuron5, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron7, input_dim = neuron6, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron8, input_dim = neuron7, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron9, input_dim = neuron8, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron10, input_dim = neuron9, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dense(neuron11, input_dim = neuron10, kernel_initializer= init, activation= activation))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            model.add(Dense(2, activation='softmax'))\n",
        "            optimizer = Adam(lr = learn_rate)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        grid = GridSearchCV(estimator = KerasClassifier(build_fn = nn_opt_10, verbose = 0),\n",
        "                            param_grid = param_grid,\n",
        "                            cv = 5,\n",
        "                            n_jobs = -1,\n",
        "                            verbose = 1)\n",
        "        grid_results = grid.fit(X_Data, to_categorical(Y_Lavel,2))\n",
        "\n",
        "        model =nn_opt_10(grid_results.best_params_['activation'],\n",
        "                grid_results.best_params_['dropout_rate'],\n",
        "                grid_results.best_params_['init'],\n",
        "                grid_results.best_params_['learn_rate'],\n",
        "                grid_results.best_params_['neuron1'],\n",
        "                grid_results.best_params_['neuron2'],\n",
        "                grid_results.best_params_['neuron3'],\n",
        "                grid_results.best_params_['neuron4'],\n",
        "                grid_results.best_params_['neuron5'],\n",
        "                grid_results.best_params_['neuron6'],\n",
        "                grid_results.best_params_['neuron7'],\n",
        "                grid_results.best_params_['neuron8'],\n",
        "                grid_results.best_params_['neuron9'],\n",
        "                grid_results.best_params_['neuron10'],\n",
        "                grid_results.best_params_['neuron11'])\n",
        "        return grid_results.best_params_['batch_size'], grid_results.best_params_['epochs'], model, grid_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#this function optimize four hyper-parameters are activation,dropout_rate,init,learn_rate\n",
        "def nn_opt(activation,dropout_rate,init,learn_rate):\n",
        "    \"\"\"\n",
        "  Parameters :\n",
        "  Input - list of 4 hyper-parameter that are need to\n",
        "  be optimized\n",
        "\n",
        "  Output - Best optimized model\n",
        "    \"\"\"\n",
        "\n",
        "    #define the optimmized neuron  number from experiment\n",
        "    neuron1,neuron2,neuron3,neuron4=64,16,64,64\n",
        "    # the model building block\n",
        "\n",
        "    model = Sequential()\n",
        "    np.random.seed(6)\n",
        "    model.add(Dense(neuron1, input_dim =4 , kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    optimizer = Adam(learning_rate = learn_rate)               #optimizer of Neural network\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) #compile model\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-40s9dkHssNu"
      },
      "source": [
        "### Helper function for Ensembling model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_BXPZol8pem"
      },
      "source": [
        "### Read the data from the drive using pandas (Python Data Analysis Library)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-q5jMU4z4GA",
        "outputId": "f3ec5db3-e1f6-4467-baeb-39cfea566baf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "if colab ==True:\n",
        "  data = pd.read_csv(data_dir)\n",
        "else:\n",
        "  data = pd.read_csv(data_dir)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "KluoAwn49hGG",
        "outputId": "a659c5ee-647f-43e1-a2cd-b5922d0b95ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "5            5      116             74              0        0  25.6   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  \n",
              "5                     0.201   30        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df785c2d-9f8a-437d-9127-3447cd220663\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df785c2d-9f8a-437d-9127-3447cd220663')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df785c2d-9f8a-437d-9127-3447cd220663 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df785c2d-9f8a-437d-9127-3447cd220663');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff55e197-133c-4779-9fb8-f0d21e7234fc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff55e197-133c-4779-9fb8-f0d21e7234fc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff55e197-133c-4779-9fb8-f0d21e7234fc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          6,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          151,\n          101,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.8841603203754405,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          19.9,\n          31.0,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33132859501277484,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 517,\n        \"samples\": [\n          1.731,\n          0.426,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "data.head(n=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwaLj2vN_RmG"
      },
      "source": [
        "### Renaming the Features by F1, F2, and so on ......\n",
        "---\n",
        "New Name | Original Name | Comments|\n",
        "---|---|---|\n",
        "F1|Pregnancies|Number of times pregnant|\n",
        "F2|Glucose|Plasma glucose concentration a 2 hours in an oral glucose tolerance test|\n",
        "F3|BloodPressure|Diastolic blood pressure (mm Hg)|\n",
        "F4|SkinThickness|Triceps skin fold thickness (mm)|\n",
        "F5|Insulin|2-Hour serum insulin (mu U/ml)|\n",
        "F6|BMI|Body mass index (weight in kg/(height in m)^2)|\n",
        "F7|DiabetesPedigreeFunction|Diabetes pedigree function|\n",
        "F8|Age|Age (years)|\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvHHG7dC_jzd"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({'F1':data.iloc[:,:8].values[:,0],\n",
        "                     'F2':data.iloc[:,:8].values[:,1],\n",
        "                     'F3':data.iloc[:,:8].values[:,2],\n",
        "                     'F4':data.iloc[:,:8].values[:,3],\n",
        "                     'F5':data.iloc[:,:8].values[:,4],\n",
        "                     'F6':data.iloc[:,:8].values[:,5],\n",
        "                     'F7':data.iloc[:,:8].values[:,6],\n",
        "                     'F8':data.iloc[:,:8].values[:,7],\n",
        "                     'Outcome':data.iloc[:,8:].values[:,0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoQ8jmU-f2A"
      },
      "source": [
        "### Show the statistical description of the data which sumarize the central tendency, dispersion, and shape of a data distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PdYuOfaHtpyn",
        "outputId": "88f20375-2836-45e7-f7ab-04251561bdf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               F1          F2          F3          F4          F5          F6  \\\n",
              "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
              "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
              "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
              "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
              "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
              "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
              "\n",
              "               F7          F8     Outcome  \n",
              "count  768.000000  768.000000  768.000000  \n",
              "mean     0.471876   33.240885    0.348958  \n",
              "std      0.331329   11.760232    0.476951  \n",
              "min      0.078000   21.000000    0.000000  \n",
              "25%      0.243750   24.000000    0.000000  \n",
              "50%      0.372500   29.000000    0.000000  \n",
              "75%      0.626250   41.000000    1.000000  \n",
              "max      2.420000   81.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61551f8f-d435-4256-8b85-3b99b8a9b4b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61551f8f-d435-4256-8b85-3b99b8a9b4b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-61551f8f-d435-4256-8b85-3b99b8a9b4b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-61551f8f-d435-4256-8b85-3b99b8a9b4b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-906b6306-0c0a-4a14-8384-befa0fdb248f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-906b6306-0c0a-4a14-8384-befa0fdb248f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-906b6306-0c0a-4a14-8384-befa0fdb248f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 269.85223453356366,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.8450520833333335,\n          3.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 243.73802348295857,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          120.89453125,\n          117.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 252.85250535810619,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          69.10546875,\n          72.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 263.7684730531098,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          768.0,\n          20.536458333333332,\n          32.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 350.26059167945886,\n        \"min\": 0.0,\n        \"max\": 846.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          768.0,\n          79.79947916666667,\n          127.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 262.05117817552093,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          31.992578124999998,\n          32.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.3005221658502,\n        \"min\": 0.078,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.47187630208333325,\n          0.3725,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 260.1941178528413,\n        \"min\": 11.76023154067868,\n        \"max\": 768.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          33.240885416666664,\n          29.0,\n          768.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.3865920388932,\n        \"min\": 0.0,\n        \"max\": 768.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3489583333333333,\n          1.0,\n          0.4769513772427971\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3dad9d1d-07d2-48fe-e5a4-022dfb83d33a",
        "id": "dRMSzvK-tXjg"
      },
      "source": [
        "# Raw Data Plot and Presenation\n",
        "**Plot pairwise relationships in a dataset:**\n",
        "* The diagonal shows the distribution of the dataset with the kernel density plots of both the classes.\n",
        "\n",
        "* The scatter-plots shows the relation between each and every attribute or features taken pairwise where the scatter-plots shows that no attributes are able to clearly distinguish the two outcome.\n",
        "\n",
        "**Distribution Plot:**\n",
        "\n",
        "Dist Plot helps us to flexibly plot a univariate distribution of observations.\n",
        "\n",
        "**Violin Plots:**\n",
        "A violin plot is a method of plotting numeric data. It is similar to box plot with a rotated kernel density plot on each side. Violin plots are similar to box plots, except that they also show the probability density of the data at different values (in the simplest case this could be a histogram).\n",
        "\n",
        "**Correlation:**\n",
        "A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "415da1b4-d02b-a94e-d289-87ff0b84d6c3",
        "id": "_wpbciDvtXji"
      },
      "outputs": [],
      "source": [
        "# data_plot (data,\n",
        "#           Pair_plot=True,\n",
        "#           Dist_Plot=True,\n",
        "#           Plot_violinplot=True,\n",
        "#           Plot_confusionMatrix=True,\n",
        "#           box_Gaussian=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7790af18-672d-658b-9d0a-58648dd63b13",
        "id": "nluf1OUetXjy"
      },
      "source": [
        "# Data Preprocessing  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr2EoFiZrUvm"
      },
      "source": [
        "### Preprocessing Selection\n",
        "---\n",
        "Process Selector | Preprocessing |\n",
        "---|---|\n",
        "P|Outlier Rejection|\n",
        "Q|Filling Missing Value|\n",
        "R|Standardization|\n",
        "\n",
        "**Each P, Q, and R process has four techniques for feature selection which are N/A, PCA, ICA, and correlation-based feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "048157d7-1cfc-13b7-feca-5c73b00a03ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbTObgkctXj1",
        "outputId": "edb4bb00-b4cc-4807-b992-abefe241bb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape After outlier Removed: (530, 9)\n",
            "Shape After Filling Missing Value: (530, 9)\n",
            "Shape After Feature Selection: (530, 4)\n"
          ]
        }
      ],
      "source": [
        "# print('Shape Before Process: ' + str(data.shape))\n",
        "\n",
        "# The process for the outlier rejection (P)\n",
        "data = outlier_Rejection (data,\n",
        "                  iqr_Mean=False,\n",
        "                  iqr_Medain=False,\n",
        "                  iqr=True,\n",
        "                  manual=False)\n",
        "print('Shape After outlier Removed: ' + str(data.shape))\n",
        "\n",
        "## The process for the filling missing values (Q)\n",
        "for col in ['F2', 'F3', 'F4', 'F5', 'F6']:\n",
        "    replace_zero(data, col, 'Outcome')\n",
        "print('Shape After Filling Missing Value: ' + str(data.shape))\n",
        "\n",
        "\n",
        "X_Data,Y_Lavel = feature_Selector(data, algo='corr', n_feature=4)\n",
        "print('Shape After Feature Selection: ' + str(X_Data.shape))\n",
        "\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5,\n",
        "                     shuffle=True,\n",
        "                     random_state=random_initializer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO-9nWCUwz--"
      },
      "source": [
        "#### Find the optimum number of hidden layers and neurons in number every layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyVnsEj3RE_M"
      },
      "outputs": [],
      "source": [
        "# Define a random seed\n",
        "seed = 6\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the model for optimization\n",
        "model = KerasClassifier(build_fn = nn_opt, verbose = 0)\n",
        "\n",
        "#List of hyper-parameters of MLP model for optimization\n",
        "\n",
        "batch_size = [8, 16, 32]\n",
        "epochs = [100, 150, 200]\n",
        "learn_rate =[0.001,.05, 0.1]\n",
        "dropout_rate = [0.0, 0.3, 0.6]\n",
        "activation = ['relu', 'tanh']\n",
        "init =['uniform', 'normal']\n",
        "\n",
        "#parameter dictionary for grid ssearch\n",
        "param_grid = dict(batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  learn_rate=learn_rate,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  activation=activation,\n",
        "                  init=init)\n",
        "\n",
        "#build and fit the GridSearchCV\n",
        "grid = GridSearchCV(estimator = model,\n",
        "                    param_grid = param_grid,\n",
        "                    cv = 5,\n",
        "                    n_jobs = -1,\n",
        "                    verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UPZJ_TGUh9Q_",
        "outputId": "082f8d22-98bf-4415-f4ad-2b38c5942114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu 8 200 0.001 0.6 normal 64 16 64 64\n",
            "------------------->>>>>>>>>>Fold no =  1\n",
            "Epoch 1/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5886 - loss: 0.6831\n",
            "Epoch 2/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.6401\n",
            "Epoch 3/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6936 - loss: 0.5742\n",
            "Epoch 4/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7464 - loss: 0.5696\n",
            "Epoch 5/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7578 - loss: 0.5711\n",
            "Epoch 6/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.5534\n",
            "Epoch 7/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8099 - loss: 0.5289\n",
            "Epoch 8/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.5151\n",
            "Epoch 9/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7814 - loss: 0.4969\n",
            "Epoch 10/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7930 - loss: 0.5091\n",
            "Epoch 11/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7653 - loss: 0.5261\n",
            "Epoch 12/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4966\n",
            "Epoch 13/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8103 - loss: 0.5110\n",
            "Epoch 14/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8311 - loss: 0.4979\n",
            "Epoch 15/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8204 - loss: 0.4653\n",
            "Epoch 16/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.4833\n",
            "Epoch 17/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8264 - loss: 0.4736\n",
            "Epoch 18/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8368 - loss: 0.4623\n",
            "Epoch 19/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4554\n",
            "Epoch 20/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8340 - loss: 0.4412\n",
            "Epoch 21/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8462 - loss: 0.4144\n",
            "Epoch 22/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8441 - loss: 0.4155\n",
            "Epoch 23/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.4436\n",
            "Epoch 24/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8512 - loss: 0.3971\n",
            "Epoch 25/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8552 - loss: 0.4101\n",
            "Epoch 26/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8350 - loss: 0.4395\n",
            "Epoch 27/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3947\n",
            "Epoch 28/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8463 - loss: 0.3931\n",
            "Epoch 29/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.4059\n",
            "Epoch 30/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8384 - loss: 0.3804\n",
            "Epoch 31/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.4164\n",
            "Epoch 32/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8705 - loss: 0.3584\n",
            "Epoch 33/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8544 - loss: 0.3718\n",
            "Epoch 34/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8714 - loss: 0.3430\n",
            "Epoch 35/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3731\n",
            "Epoch 36/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8486 - loss: 0.3627\n",
            "Epoch 37/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8699 - loss: 0.3810\n",
            "Epoch 38/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8607 - loss: 0.3860\n",
            "Epoch 39/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8624 - loss: 0.3693\n",
            "Epoch 40/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8849 - loss: 0.3278\n",
            "Epoch 41/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3523\n",
            "Epoch 42/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.3696\n",
            "Epoch 43/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8664 - loss: 0.3666\n",
            "Epoch 44/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.3514\n",
            "Epoch 45/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8591 - loss: 0.3806\n",
            "Epoch 46/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.3497\n",
            "Epoch 47/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8682 - loss: 0.3723\n",
            "Epoch 48/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8850 - loss: 0.3498\n",
            "Epoch 49/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 0.3561\n",
            "Epoch 50/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3540\n",
            "Epoch 51/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.3444\n",
            "Epoch 52/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8888 - loss: 0.3378\n",
            "Epoch 53/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8571 - loss: 0.3974\n",
            "Epoch 54/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8603 - loss: 0.3439\n",
            "Epoch 55/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.3308\n",
            "Epoch 56/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.3535\n",
            "Epoch 57/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3425\n",
            "Epoch 58/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3681\n",
            "Epoch 59/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8650 - loss: 0.3291\n",
            "Epoch 60/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8832 - loss: 0.3319\n",
            "Epoch 61/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8657 - loss: 0.3653\n",
            "Epoch 62/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8784 - loss: 0.3255\n",
            "Epoch 63/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8798 - loss: 0.3584\n",
            "Epoch 64/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8745 - loss: 0.3389\n",
            "Epoch 65/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8706 - loss: 0.3362\n",
            "Epoch 66/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8907 - loss: 0.3548\n",
            "Epoch 67/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.3500\n",
            "Epoch 68/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8861 - loss: 0.3397\n",
            "Epoch 69/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8734 - loss: 0.3236\n",
            "Epoch 70/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8893 - loss: 0.3431\n",
            "Epoch 71/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8740 - loss: 0.3354\n",
            "Epoch 72/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8913 - loss: 0.3373\n",
            "Epoch 73/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3704\n",
            "Epoch 74/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3327\n",
            "Epoch 75/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.3402\n",
            "Epoch 76/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8813 - loss: 0.3497\n",
            "Epoch 77/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8893 - loss: 0.3202\n",
            "Epoch 78/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8774 - loss: 0.3261\n",
            "Epoch 79/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.3262\n",
            "Epoch 80/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8874 - loss: 0.3451\n",
            "Epoch 81/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8737 - loss: 0.3424\n",
            "Epoch 82/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8924 - loss: 0.3370\n",
            "Epoch 83/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8828 - loss: 0.3489\n",
            "Epoch 84/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8651 - loss: 0.3345\n",
            "Epoch 85/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.3433\n",
            "Epoch 86/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8841 - loss: 0.3336\n",
            "Epoch 87/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8877 - loss: 0.3227\n",
            "Epoch 88/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8903 - loss: 0.3350\n",
            "Epoch 89/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8808 - loss: 0.3442\n",
            "Epoch 90/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8982 - loss: 0.3035\n",
            "Epoch 91/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8928 - loss: 0.3398\n",
            "Epoch 92/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8852 - loss: 0.3589\n",
            "Epoch 93/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.3338\n",
            "Epoch 94/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8645 - loss: 0.3606\n",
            "Epoch 95/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8877 - loss: 0.3209\n",
            "Epoch 96/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8798 - loss: 0.3479\n",
            "Epoch 97/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8814 - loss: 0.3387\n",
            "Epoch 98/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.3406\n",
            "Epoch 99/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8857 - loss: 0.3081\n",
            "Epoch 100/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8815 - loss: 0.3441\n",
            "Epoch 101/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8881 - loss: 0.3254\n",
            "Epoch 102/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8989 - loss: 0.3062\n",
            "Epoch 103/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.3183\n",
            "Epoch 104/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8844 - loss: 0.3131\n",
            "Epoch 105/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8905 - loss: 0.3192\n",
            "Epoch 106/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8907 - loss: 0.3343\n",
            "Epoch 107/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8822 - loss: 0.3303\n",
            "Epoch 108/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.3275\n",
            "Epoch 109/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8848 - loss: 0.3074\n",
            "Epoch 110/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8924 - loss: 0.3227\n",
            "Epoch 111/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8969 - loss: 0.2732\n",
            "Epoch 112/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.2962\n",
            "Epoch 113/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9027 - loss: 0.2860\n",
            "Epoch 114/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9139 - loss: 0.3043\n",
            "Epoch 115/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9022 - loss: 0.3014\n",
            "Epoch 116/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8983 - loss: 0.3123\n",
            "Epoch 117/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9087 - loss: 0.2945\n",
            "Epoch 118/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8970 - loss: 0.3261\n",
            "Epoch 119/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9014 - loss: 0.3201\n",
            "Epoch 120/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9035 - loss: 0.3095\n",
            "Epoch 121/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.3158\n",
            "Epoch 122/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8854 - loss: 0.3014\n",
            "Epoch 123/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8996 - loss: 0.3325\n",
            "Epoch 124/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8786 - loss: 0.3129\n",
            "Epoch 125/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9036 - loss: 0.2912\n",
            "Epoch 126/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8999 - loss: 0.2895\n",
            "Epoch 127/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9072 - loss: 0.3001\n",
            "Epoch 128/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9014 - loss: 0.3251\n",
            "Epoch 129/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8889 - loss: 0.3228\n",
            "Epoch 130/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8976 - loss: 0.2987\n",
            "Epoch 131/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9104 - loss: 0.3213\n",
            "Epoch 132/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8819 - loss: 0.3250\n",
            "Epoch 133/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.3065\n",
            "Epoch 134/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9017 - loss: 0.2967\n",
            "Epoch 135/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9033 - loss: 0.3079\n",
            "Epoch 136/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.3038\n",
            "Epoch 137/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8948 - loss: 0.3088\n",
            "Epoch 138/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8932 - loss: 0.3070\n",
            "Epoch 139/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8898 - loss: 0.3103\n",
            "Epoch 140/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8983 - loss: 0.3224\n",
            "Epoch 141/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8995 - loss: 0.3023\n",
            "Epoch 142/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8947 - loss: 0.2927\n",
            "Epoch 143/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8881 - loss: 0.2837\n",
            "Epoch 144/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8892 - loss: 0.3018\n",
            "Epoch 145/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8790 - loss: 0.3049\n",
            "Epoch 146/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8906 - loss: 0.2989\n",
            "Epoch 147/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8825 - loss: 0.3014\n",
            "Epoch 148/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9070 - loss: 0.2948\n",
            "Epoch 149/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9016 - loss: 0.2984\n",
            "Epoch 150/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9058 - loss: 0.3217\n",
            "Epoch 151/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9105 - loss: 0.2837\n",
            "Epoch 152/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9044 - loss: 0.2683\n",
            "Epoch 153/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9060 - loss: 0.2882\n",
            "Epoch 154/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9016 - loss: 0.2726\n",
            "Epoch 155/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8973 - loss: 0.3345\n",
            "Epoch 156/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9035 - loss: 0.3187\n",
            "Epoch 157/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8785 - loss: 0.3022\n",
            "Epoch 158/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.3187\n",
            "Epoch 159/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8895 - loss: 0.2976\n",
            "Epoch 160/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.2892\n",
            "Epoch 161/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9128 - loss: 0.2714\n",
            "Epoch 162/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8842 - loss: 0.3048\n",
            "Epoch 163/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8851 - loss: 0.2982\n",
            "Epoch 164/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9013 - loss: 0.2756\n",
            "Epoch 165/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9089 - loss: 0.2678\n",
            "Epoch 166/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9109 - loss: 0.3382\n",
            "Epoch 167/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8858 - loss: 0.2735\n",
            "Epoch 168/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9023 - loss: 0.2938\n",
            "Epoch 169/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9009 - loss: 0.2886\n",
            "Epoch 170/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8999 - loss: 0.2766\n",
            "Epoch 171/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8860 - loss: 0.3055\n",
            "Epoch 172/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.2819\n",
            "Epoch 173/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9078 - loss: 0.2687\n",
            "Epoch 174/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8864 - loss: 0.2674\n",
            "Epoch 175/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9058 - loss: 0.2909\n",
            "Epoch 176/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9034 - loss: 0.2773\n",
            "Epoch 177/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9117 - loss: 0.2709\n",
            "Epoch 178/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8783 - loss: 0.2829\n",
            "Epoch 179/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8603 - loss: 0.3391\n",
            "Epoch 180/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8942 - loss: 0.2891\n",
            "Epoch 181/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2831\n",
            "Epoch 182/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9017 - loss: 0.2714\n",
            "Epoch 183/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9059 - loss: 0.2651\n",
            "Epoch 184/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8878 - loss: 0.3114\n",
            "Epoch 185/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8985 - loss: 0.3011\n",
            "Epoch 186/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9051 - loss: 0.2773\n",
            "Epoch 187/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9115 - loss: 0.2891\n",
            "Epoch 188/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8936 - loss: 0.2902\n",
            "Epoch 189/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9059 - loss: 0.2727\n",
            "Epoch 190/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9124 - loss: 0.2883\n",
            "Epoch 191/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9161 - loss: 0.2626\n",
            "Epoch 192/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.2824\n",
            "Epoch 193/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.2696\n",
            "Epoch 194/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9097 - loss: 0.2889\n",
            "Epoch 195/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.2625\n",
            "Epoch 196/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8696 - loss: 0.3405\n",
            "Epoch 197/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8844 - loss: 0.2867\n",
            "Epoch 198/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9089 - loss: 0.2669\n",
            "Epoch 199/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9077 - loss: 0.2593\n",
            "Epoch 200/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8805 - loss: 0.3165\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87        75\n",
            "           1       0.68      0.68      0.68        31\n",
            "\n",
            "    accuracy                           0.81       106\n",
            "   macro avg       0.77      0.77      0.77       106\n",
            "weighted avg       0.81      0.81      0.81       106\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8761290322580646\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[65 10]\n",
            " [10 21]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8113207547169812\n",
            "\n",
            "13.65\n",
            "------------------->>>>>>>>>>Fold no =  2\n",
            "Epoch 1/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6172 - loss: 0.6711\n",
            "Epoch 2/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.6507\n",
            "Epoch 3/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6601 - loss: 0.6103\n",
            "Epoch 4/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7395 - loss: 0.5785\n",
            "Epoch 5/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7110 - loss: 0.5782\n",
            "Epoch 6/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6946 - loss: 0.5908\n",
            "Epoch 7/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5696\n",
            "Epoch 8/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7588 - loss: 0.5431\n",
            "Epoch 9/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.5187\n",
            "Epoch 10/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7801 - loss: 0.5427\n",
            "Epoch 11/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7765 - loss: 0.5499\n",
            "Epoch 12/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.5799\n",
            "Epoch 13/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.5130\n",
            "Epoch 14/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7809 - loss: 0.5236\n",
            "Epoch 15/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8186 - loss: 0.4946\n",
            "Epoch 16/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8031 - loss: 0.5229\n",
            "Epoch 17/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8056 - loss: 0.4813\n",
            "Epoch 18/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8019 - loss: 0.5039\n",
            "Epoch 19/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8192 - loss: 0.5137\n",
            "Epoch 20/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8031 - loss: 0.5149\n",
            "Epoch 21/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8129 - loss: 0.4679\n",
            "Epoch 22/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8101 - loss: 0.4999\n",
            "Epoch 23/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.4872\n",
            "Epoch 24/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8051 - loss: 0.4839\n",
            "Epoch 25/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8360 - loss: 0.4862\n",
            "Epoch 26/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.4593\n",
            "Epoch 27/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4813\n",
            "Epoch 28/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8210 - loss: 0.4477\n",
            "Epoch 29/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.4673\n",
            "Epoch 30/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8262 - loss: 0.4497\n",
            "Epoch 31/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8385 - loss: 0.4477\n",
            "Epoch 32/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8390 - loss: 0.4047\n",
            "Epoch 33/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8444 - loss: 0.4572\n",
            "Epoch 34/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8303 - loss: 0.4276\n",
            "Epoch 35/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.4660\n",
            "Epoch 36/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8403 - loss: 0.4708\n",
            "Epoch 37/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8375 - loss: 0.4711\n",
            "Epoch 38/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8718 - loss: 0.4030\n",
            "Epoch 39/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8297 - loss: 0.4160\n",
            "Epoch 40/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8358 - loss: 0.4395\n",
            "Epoch 41/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8527 - loss: 0.4172\n",
            "Epoch 42/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8252 - loss: 0.4521\n",
            "Epoch 43/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8403 - loss: 0.4317\n",
            "Epoch 44/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8275 - loss: 0.4773\n",
            "Epoch 45/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.4248\n",
            "Epoch 46/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.4393\n",
            "Epoch 47/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8374 - loss: 0.4416\n",
            "Epoch 48/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8523 - loss: 0.4240\n",
            "Epoch 49/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.4531\n",
            "Epoch 50/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.4073\n",
            "Epoch 51/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8552 - loss: 0.4332\n",
            "Epoch 52/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8487 - loss: 0.4206\n",
            "Epoch 53/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.4350\n",
            "Epoch 54/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8596 - loss: 0.4317\n",
            "Epoch 55/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8778 - loss: 0.4064\n",
            "Epoch 56/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8679 - loss: 0.4087\n",
            "Epoch 57/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.4307\n",
            "Epoch 58/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.4151\n",
            "Epoch 59/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8820 - loss: 0.3791\n",
            "Epoch 60/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8415 - loss: 0.4365\n",
            "Epoch 61/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8738 - loss: 0.3818\n",
            "Epoch 62/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.3988\n",
            "Epoch 63/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8701 - loss: 0.4330\n",
            "Epoch 64/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8673 - loss: 0.3852\n",
            "Epoch 65/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.4040\n",
            "Epoch 66/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8656 - loss: 0.4013\n",
            "Epoch 67/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8705 - loss: 0.3973\n",
            "Epoch 68/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.3599\n",
            "Epoch 69/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8723 - loss: 0.3897\n",
            "Epoch 70/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8582 - loss: 0.4205\n",
            "Epoch 71/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8723 - loss: 0.4075\n",
            "Epoch 72/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8513 - loss: 0.4161\n",
            "Epoch 73/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8508 - loss: 0.3882\n",
            "Epoch 74/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8714 - loss: 0.4061\n",
            "Epoch 75/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8788 - loss: 0.3707\n",
            "Epoch 76/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8613 - loss: 0.3780\n",
            "Epoch 77/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8536 - loss: 0.3890\n",
            "Epoch 78/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8508 - loss: 0.4541\n",
            "Epoch 79/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8662 - loss: 0.4041\n",
            "Epoch 80/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8631 - loss: 0.3726\n",
            "Epoch 81/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8761 - loss: 0.3621\n",
            "Epoch 82/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8674 - loss: 0.3991\n",
            "Epoch 83/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.3808\n",
            "Epoch 84/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8727 - loss: 0.3613\n",
            "Epoch 85/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8699 - loss: 0.3659\n",
            "Epoch 86/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.4238\n",
            "Epoch 87/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3760\n",
            "Epoch 88/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8797 - loss: 0.3468\n",
            "Epoch 89/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8718 - loss: 0.3484\n",
            "Epoch 90/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.4156\n",
            "Epoch 91/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8782 - loss: 0.4094\n",
            "Epoch 92/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8670 - loss: 0.3648\n",
            "Epoch 93/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8740 - loss: 0.3468\n",
            "Epoch 94/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8644 - loss: 0.3839\n",
            "Epoch 95/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8754 - loss: 0.3678\n",
            "Epoch 96/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8853 - loss: 0.3531\n",
            "Epoch 97/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8727 - loss: 0.3939\n",
            "Epoch 98/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8786 - loss: 0.3750\n",
            "Epoch 99/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8681 - loss: 0.4039\n",
            "Epoch 100/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8742 - loss: 0.3518\n",
            "Epoch 101/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8838 - loss: 0.3591\n",
            "Epoch 102/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3720\n",
            "Epoch 103/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.3676\n",
            "Epoch 104/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8681 - loss: 0.3669\n",
            "Epoch 105/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8757 - loss: 0.3680\n",
            "Epoch 106/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8760 - loss: 0.4010\n",
            "Epoch 107/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3527\n",
            "Epoch 108/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8682 - loss: 0.3870\n",
            "Epoch 109/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8654 - loss: 0.3815\n",
            "Epoch 110/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.3279\n",
            "Epoch 111/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.3255\n",
            "Epoch 112/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3590\n",
            "Epoch 113/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8857 - loss: 0.3514\n",
            "Epoch 114/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8850 - loss: 0.3635\n",
            "Epoch 115/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8651 - loss: 0.3793\n",
            "Epoch 116/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8670 - loss: 0.3342\n",
            "Epoch 117/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8809 - loss: 0.3697\n",
            "Epoch 118/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8552 - loss: 0.3540\n",
            "Epoch 119/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8624 - loss: 0.4109\n",
            "Epoch 120/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.4003\n",
            "Epoch 121/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8672 - loss: 0.3757\n",
            "Epoch 122/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8801 - loss: 0.3777\n",
            "Epoch 123/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8742 - loss: 0.3754\n",
            "Epoch 124/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8710 - loss: 0.3707\n",
            "Epoch 125/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8716 - loss: 0.3530\n",
            "Epoch 126/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8711 - loss: 0.3204\n",
            "Epoch 127/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8739 - loss: 0.3531\n",
            "Epoch 128/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3377\n",
            "Epoch 129/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8575 - loss: 0.3987\n",
            "Epoch 130/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8524 - loss: 0.3850\n",
            "Epoch 131/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8891 - loss: 0.3581\n",
            "Epoch 132/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.3377\n",
            "Epoch 133/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.3280\n",
            "Epoch 134/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8714 - loss: 0.3647\n",
            "Epoch 135/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 0.3583\n",
            "Epoch 136/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.3391\n",
            "Epoch 137/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8821 - loss: 0.3076\n",
            "Epoch 138/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.3902\n",
            "Epoch 139/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8647 - loss: 0.3529\n",
            "Epoch 140/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8809 - loss: 0.3621\n",
            "Epoch 141/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8744 - loss: 0.3342\n",
            "Epoch 142/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.3755\n",
            "Epoch 143/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3148\n",
            "Epoch 144/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.3258\n",
            "Epoch 145/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8785 - loss: 0.3357\n",
            "Epoch 146/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.3327\n",
            "Epoch 147/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8853 - loss: 0.3220\n",
            "Epoch 148/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.3411\n",
            "Epoch 149/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8733 - loss: 0.3365\n",
            "Epoch 150/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8874 - loss: 0.3252\n",
            "Epoch 151/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8607 - loss: 0.3690\n",
            "Epoch 152/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8847 - loss: 0.3762\n",
            "Epoch 153/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 0.2986\n",
            "Epoch 154/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8761 - loss: 0.3265\n",
            "Epoch 155/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8904 - loss: 0.3142\n",
            "Epoch 156/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3326\n",
            "Epoch 157/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8802 - loss: 0.3245\n",
            "Epoch 158/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.3333\n",
            "Epoch 159/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8655 - loss: 0.3333\n",
            "Epoch 160/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8827 - loss: 0.3546\n",
            "Epoch 161/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8715 - loss: 0.3444\n",
            "Epoch 162/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8828 - loss: 0.3474\n",
            "Epoch 163/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.3486\n",
            "Epoch 164/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8718 - loss: 0.3707\n",
            "Epoch 165/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8894 - loss: 0.3158\n",
            "Epoch 166/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.3234\n",
            "Epoch 167/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.3184\n",
            "Epoch 168/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8725 - loss: 0.3100\n",
            "Epoch 169/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8798 - loss: 0.3241\n",
            "Epoch 170/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8883 - loss: 0.3281\n",
            "Epoch 171/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.3753\n",
            "Epoch 172/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8858 - loss: 0.3187\n",
            "Epoch 173/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8794 - loss: 0.3584\n",
            "Epoch 174/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8867 - loss: 0.3101\n",
            "Epoch 175/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8781 - loss: 0.3231\n",
            "Epoch 176/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8784 - loss: 0.2959\n",
            "Epoch 177/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8776 - loss: 0.3288\n",
            "Epoch 178/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8845 - loss: 0.3230\n",
            "Epoch 179/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8732 - loss: 0.3507\n",
            "Epoch 180/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8762 - loss: 0.3205\n",
            "Epoch 181/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8904 - loss: 0.2950\n",
            "Epoch 182/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8763 - loss: 0.3327\n",
            "Epoch 183/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8618 - loss: 0.3460\n",
            "Epoch 184/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8873 - loss: 0.3213\n",
            "Epoch 185/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8938 - loss: 0.3289\n",
            "Epoch 186/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8845 - loss: 0.3398\n",
            "Epoch 187/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8828 - loss: 0.3145\n",
            "Epoch 188/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.3359\n",
            "Epoch 189/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.3021\n",
            "Epoch 190/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.3002\n",
            "Epoch 191/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8913 - loss: 0.3017\n",
            "Epoch 192/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8919 - loss: 0.3107\n",
            "Epoch 193/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8875 - loss: 0.2963\n",
            "Epoch 194/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8844 - loss: 0.2901\n",
            "Epoch 195/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8830 - loss: 0.3334\n",
            "Epoch 196/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8707 - loss: 0.3212\n",
            "Epoch 197/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8875 - loss: 0.2926\n",
            "Epoch 198/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8704 - loss: 0.3415\n",
            "Epoch 199/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8963 - loss: 0.3077\n",
            "Epoch 200/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8646 - loss: 0.3696\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90        75\n",
            "           1       0.77      0.74      0.75        31\n",
            "\n",
            "    accuracy                           0.86       106\n",
            "   macro avg       0.83      0.82      0.83       106\n",
            "weighted avg       0.86      0.86      0.86       106\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.901505376344086\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[68  7]\n",
            " [ 8 23]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8584905660377359\n",
            "\n",
            "27.928571428571427\n",
            "------------------->>>>>>>>>>Fold no =  3\n",
            "Epoch 1/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6505 - loss: 0.6658\n",
            "Epoch 2/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6970 - loss: 0.6466\n",
            "Epoch 3/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6970 - loss: 0.6076\n",
            "Epoch 4/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6972 - loss: 0.5856\n",
            "Epoch 5/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6621 - loss: 0.6253\n",
            "Epoch 6/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7247 - loss: 0.5798\n",
            "Epoch 7/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7103 - loss: 0.5740\n",
            "Epoch 8/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7017 - loss: 0.5694\n",
            "Epoch 9/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7416 - loss: 0.5576\n",
            "Epoch 10/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7659 - loss: 0.5344\n",
            "Epoch 11/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7254 - loss: 0.5537\n",
            "Epoch 12/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5534\n",
            "Epoch 13/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.5274\n",
            "Epoch 14/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.5486\n",
            "Epoch 15/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7475 - loss: 0.5318\n",
            "Epoch 16/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7665 - loss: 0.5178\n",
            "Epoch 17/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7986 - loss: 0.5171\n",
            "Epoch 18/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7899 - loss: 0.5277\n",
            "Epoch 19/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7874 - loss: 0.5299\n",
            "Epoch 20/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7600 - loss: 0.5262\n",
            "Epoch 21/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.5127\n",
            "Epoch 22/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7820 - loss: 0.5215\n",
            "Epoch 23/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.4901\n",
            "Epoch 24/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4893\n",
            "Epoch 25/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8089 - loss: 0.5010\n",
            "Epoch 26/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.4767\n",
            "Epoch 27/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4446\n",
            "Epoch 28/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7856 - loss: 0.5058\n",
            "Epoch 29/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8046 - loss: 0.4766\n",
            "Epoch 30/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8083 - loss: 0.4619\n",
            "Epoch 31/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4712\n",
            "Epoch 32/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.4731\n",
            "Epoch 33/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8430 - loss: 0.3940\n",
            "Epoch 34/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4418\n",
            "Epoch 35/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8373 - loss: 0.4367\n",
            "Epoch 36/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8138 - loss: 0.4486\n",
            "Epoch 37/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8187 - loss: 0.4674\n",
            "Epoch 38/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8528 - loss: 0.4081\n",
            "Epoch 39/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.3989\n",
            "Epoch 40/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.3909\n",
            "Epoch 41/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8477 - loss: 0.4180\n",
            "Epoch 42/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8215 - loss: 0.4157\n",
            "Epoch 43/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8051 - loss: 0.4456\n",
            "Epoch 44/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8509 - loss: 0.4055\n",
            "Epoch 45/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8537 - loss: 0.3916\n",
            "Epoch 46/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8388 - loss: 0.4160\n",
            "Epoch 47/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.4014\n",
            "Epoch 48/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8336 - loss: 0.3678\n",
            "Epoch 49/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8455 - loss: 0.4026\n",
            "Epoch 50/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.4011\n",
            "Epoch 51/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8447 - loss: 0.3990\n",
            "Epoch 52/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8377 - loss: 0.3939\n",
            "Epoch 53/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.4259\n",
            "Epoch 54/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.3746\n",
            "Epoch 55/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.3621\n",
            "Epoch 56/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3806\n",
            "Epoch 57/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7950 - loss: 0.4561\n",
            "Epoch 58/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8155 - loss: 0.4105\n",
            "Epoch 59/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8579 - loss: 0.3825\n",
            "Epoch 60/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8288 - loss: 0.3957\n",
            "Epoch 61/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8237 - loss: 0.3726\n",
            "Epoch 62/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8372 - loss: 0.4055\n",
            "Epoch 63/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8627 - loss: 0.3623\n",
            "Epoch 64/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8321 - loss: 0.3892\n",
            "Epoch 65/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7855 - loss: 0.4718\n",
            "Epoch 66/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8463 - loss: 0.4092\n",
            "Epoch 67/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8199 - loss: 0.3972\n",
            "Epoch 68/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8237 - loss: 0.4030\n",
            "Epoch 69/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8184 - loss: 0.3960\n",
            "Epoch 70/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8244 - loss: 0.4100\n",
            "Epoch 71/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8563 - loss: 0.3618\n",
            "Epoch 72/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8597 - loss: 0.3921\n",
            "Epoch 73/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3354\n",
            "Epoch 74/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8589 - loss: 0.3759\n",
            "Epoch 75/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.4148\n",
            "Epoch 76/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8694 - loss: 0.3572\n",
            "Epoch 77/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8585 - loss: 0.3499\n",
            "Epoch 78/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8622 - loss: 0.3985\n",
            "Epoch 79/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8707 - loss: 0.3647\n",
            "Epoch 80/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4083\n",
            "Epoch 81/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.3967\n",
            "Epoch 82/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8583 - loss: 0.3877\n",
            "Epoch 83/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3738\n",
            "Epoch 84/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8452 - loss: 0.3526\n",
            "Epoch 85/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8604 - loss: 0.3765\n",
            "Epoch 86/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8470 - loss: 0.3922\n",
            "Epoch 87/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.3706\n",
            "Epoch 88/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8460 - loss: 0.3682\n",
            "Epoch 89/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.3502\n",
            "Epoch 90/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8516 - loss: 0.3938\n",
            "Epoch 91/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8480 - loss: 0.3752\n",
            "Epoch 92/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8488 - loss: 0.3684\n",
            "Epoch 93/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8556 - loss: 0.3597\n",
            "Epoch 94/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8526 - loss: 0.3521\n",
            "Epoch 95/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.3301\n",
            "Epoch 96/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8549 - loss: 0.3663\n",
            "Epoch 97/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8676 - loss: 0.3574\n",
            "Epoch 98/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8632 - loss: 0.3842\n",
            "Epoch 99/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8483 - loss: 0.3838\n",
            "Epoch 100/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8389 - loss: 0.3910\n",
            "Epoch 101/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8578 - loss: 0.3395\n",
            "Epoch 102/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8382 - loss: 0.3849\n",
            "Epoch 103/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8610 - loss: 0.3871\n",
            "Epoch 104/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.3628\n",
            "Epoch 105/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.3627\n",
            "Epoch 106/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8519 - loss: 0.3687\n",
            "Epoch 107/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8743 - loss: 0.3625\n",
            "Epoch 108/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8546 - loss: 0.3238\n",
            "Epoch 109/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8526 - loss: 0.3643\n",
            "Epoch 110/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.3316\n",
            "Epoch 111/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8474 - loss: 0.3438\n",
            "Epoch 112/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8574 - loss: 0.3612\n",
            "Epoch 113/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8644 - loss: 0.3701\n",
            "Epoch 114/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8661 - loss: 0.3719\n",
            "Epoch 115/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.3880\n",
            "Epoch 116/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8311 - loss: 0.3780\n",
            "Epoch 117/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.3546\n",
            "Epoch 118/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.3882\n",
            "Epoch 119/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8545 - loss: 0.3507\n",
            "Epoch 120/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8772 - loss: 0.3291\n",
            "Epoch 121/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.3950\n",
            "Epoch 122/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8613 - loss: 0.3337\n",
            "Epoch 123/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.3426\n",
            "Epoch 124/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8534 - loss: 0.3435\n",
            "Epoch 125/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8712 - loss: 0.3774\n",
            "Epoch 126/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8536 - loss: 0.3417\n",
            "Epoch 127/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8681 - loss: 0.3280\n",
            "Epoch 128/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.3260\n",
            "Epoch 129/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8650 - loss: 0.3060\n",
            "Epoch 130/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.3427\n",
            "Epoch 131/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.3141\n",
            "Epoch 132/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8733 - loss: 0.3494\n",
            "Epoch 133/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8553 - loss: 0.3467\n",
            "Epoch 134/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8532 - loss: 0.3695\n",
            "Epoch 135/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.3608\n",
            "Epoch 136/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8048 - loss: 0.4229\n",
            "Epoch 137/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8370 - loss: 0.3834\n",
            "Epoch 138/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.3828\n",
            "Epoch 139/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3429\n",
            "Epoch 140/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8563 - loss: 0.3355\n",
            "Epoch 141/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8686 - loss: 0.3305\n",
            "Epoch 142/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.3456\n",
            "Epoch 143/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8803 - loss: 0.3493\n",
            "Epoch 144/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8652 - loss: 0.3366\n",
            "Epoch 145/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8707 - loss: 0.3339\n",
            "Epoch 146/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8855 - loss: 0.3399\n",
            "Epoch 147/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8780 - loss: 0.3314\n",
            "Epoch 148/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8563 - loss: 0.3601\n",
            "Epoch 149/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8617 - loss: 0.3236\n",
            "Epoch 150/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.3286\n",
            "Epoch 151/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8629 - loss: 0.3801\n",
            "Epoch 152/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3222\n",
            "Epoch 153/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8761 - loss: 0.3224\n",
            "Epoch 154/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8624 - loss: 0.3394\n",
            "Epoch 155/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.3363\n",
            "Epoch 156/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8688 - loss: 0.3332\n",
            "Epoch 157/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8661 - loss: 0.3406\n",
            "Epoch 158/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8810 - loss: 0.3249\n",
            "Epoch 159/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8688 - loss: 0.3558\n",
            "Epoch 160/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3355\n",
            "Epoch 161/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.3100\n",
            "Epoch 162/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8884 - loss: 0.3150\n",
            "Epoch 163/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3559\n",
            "Epoch 164/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.3330\n",
            "Epoch 165/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8759 - loss: 0.3300\n",
            "Epoch 166/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8818 - loss: 0.3139\n",
            "Epoch 167/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.3395\n",
            "Epoch 168/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.3074\n",
            "Epoch 169/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8576 - loss: 0.3669\n",
            "Epoch 170/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3111\n",
            "Epoch 171/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.3363\n",
            "Epoch 172/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8852 - loss: 0.3046\n",
            "Epoch 173/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8667 - loss: 0.3154\n",
            "Epoch 174/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8744 - loss: 0.3311\n",
            "Epoch 175/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8648 - loss: 0.3127\n",
            "Epoch 176/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8615 - loss: 0.3385\n",
            "Epoch 177/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8575 - loss: 0.3290\n",
            "Epoch 178/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8582 - loss: 0.3226\n",
            "Epoch 179/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8869 - loss: 0.3218\n",
            "Epoch 180/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8763 - loss: 0.3140\n",
            "Epoch 181/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8908 - loss: 0.3024\n",
            "Epoch 182/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8654 - loss: 0.3217\n",
            "Epoch 183/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8689 - loss: 0.3128\n",
            "Epoch 184/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.3123\n",
            "Epoch 185/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8739 - loss: 0.2960\n",
            "Epoch 186/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3322\n",
            "Epoch 187/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.3276\n",
            "Epoch 188/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8762 - loss: 0.3111\n",
            "Epoch 189/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8895 - loss: 0.3052\n",
            "Epoch 190/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8554 - loss: 0.3268\n",
            "Epoch 191/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8885 - loss: 0.2925\n",
            "Epoch 192/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.3207\n",
            "Epoch 193/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8867 - loss: 0.3038\n",
            "Epoch 194/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8837 - loss: 0.3183\n",
            "Epoch 195/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.3351\n",
            "Epoch 196/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8824 - loss: 0.3093\n",
            "Epoch 197/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8826 - loss: 0.2916\n",
            "Epoch 198/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.3081\n",
            "Epoch 199/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8853 - loss: 0.2620\n",
            "Epoch 200/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.3047\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91        75\n",
            "           1       0.76      0.81      0.78        31\n",
            "\n",
            "    accuracy                           0.87       106\n",
            "   macro avg       0.84      0.85      0.84       106\n",
            "weighted avg       0.87      0.87      0.87       106\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.9255913978494623\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[67  8]\n",
            " [ 6 25]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8679245283018868\n",
            "\n",
            "34.895833333333336\n",
            "------------------->>>>>>>>>>Fold no =  4\n",
            "Epoch 1/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6089 - loss: 0.6743\n",
            "Epoch 2/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6539\n",
            "Epoch 3/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6249\n",
            "Epoch 4/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6097\n",
            "Epoch 5/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6003\n",
            "Epoch 6/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5992\n",
            "Epoch 7/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6033\n",
            "Epoch 8/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5905\n",
            "Epoch 9/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5811\n",
            "Epoch 10/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5704\n",
            "Epoch 11/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5795\n",
            "Epoch 12/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5817\n",
            "Epoch 13/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6894 - loss: 0.5930\n",
            "Epoch 14/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.5741\n",
            "Epoch 15/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6885 - loss: 0.5648\n",
            "Epoch 16/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7060 - loss: 0.5803\n",
            "Epoch 17/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7351 - loss: 0.5555\n",
            "Epoch 18/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6747 - loss: 0.5614\n",
            "Epoch 19/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7229 - loss: 0.5543\n",
            "Epoch 20/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7140 - loss: 0.5704\n",
            "Epoch 21/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7428 - loss: 0.5375\n",
            "Epoch 22/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7337 - loss: 0.5575\n",
            "Epoch 23/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6616 - loss: 0.5461\n",
            "Epoch 24/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7504 - loss: 0.5329\n",
            "Epoch 25/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7391 - loss: 0.5338\n",
            "Epoch 26/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7279 - loss: 0.5472\n",
            "Epoch 27/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7634 - loss: 0.5201\n",
            "Epoch 28/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7299 - loss: 0.5506\n",
            "Epoch 29/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7198 - loss: 0.5564\n",
            "Epoch 30/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7111 - loss: 0.5445\n",
            "Epoch 31/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.5644\n",
            "Epoch 32/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6702 - loss: 0.5582\n",
            "Epoch 33/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6794 - loss: 0.5526\n",
            "Epoch 34/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6618 - loss: 0.5451\n",
            "Epoch 35/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.5651\n",
            "Epoch 36/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7166 - loss: 0.5436\n",
            "Epoch 37/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.5383\n",
            "Epoch 38/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.5180\n",
            "Epoch 39/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7487 - loss: 0.5265\n",
            "Epoch 40/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7490 - loss: 0.5347\n",
            "Epoch 41/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7197 - loss: 0.5359\n",
            "Epoch 42/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7414 - loss: 0.5233\n",
            "Epoch 43/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7551 - loss: 0.5149\n",
            "Epoch 44/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7698 - loss: 0.4855\n",
            "Epoch 45/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.4855\n",
            "Epoch 46/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7864 - loss: 0.4884\n",
            "Epoch 47/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7812 - loss: 0.4921\n",
            "Epoch 48/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.4755\n",
            "Epoch 49/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8352 - loss: 0.4434\n",
            "Epoch 50/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8369 - loss: 0.4336\n",
            "Epoch 51/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8498 - loss: 0.4193\n",
            "Epoch 52/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8499 - loss: 0.3933\n",
            "Epoch 53/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8484 - loss: 0.4205\n",
            "Epoch 54/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8580 - loss: 0.4013\n",
            "Epoch 55/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8459 - loss: 0.3990\n",
            "Epoch 56/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3892\n",
            "Epoch 57/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8518 - loss: 0.4057\n",
            "Epoch 58/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.3956\n",
            "Epoch 59/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8559 - loss: 0.3810\n",
            "Epoch 60/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.3638\n",
            "Epoch 61/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8742 - loss: 0.3343\n",
            "Epoch 62/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8539 - loss: 0.3615\n",
            "Epoch 63/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8272 - loss: 0.3766\n",
            "Epoch 64/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8720 - loss: 0.3525\n",
            "Epoch 65/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8701 - loss: 0.3653\n",
            "Epoch 66/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8843 - loss: 0.3637\n",
            "Epoch 67/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8784 - loss: 0.3611\n",
            "Epoch 68/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8738 - loss: 0.3689\n",
            "Epoch 69/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8606 - loss: 0.3694\n",
            "Epoch 70/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8849 - loss: 0.3446\n",
            "Epoch 71/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8802 - loss: 0.3378\n",
            "Epoch 72/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8776 - loss: 0.3605\n",
            "Epoch 73/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3406\n",
            "Epoch 74/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8927 - loss: 0.3499\n",
            "Epoch 75/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8717 - loss: 0.3634\n",
            "Epoch 76/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8872 - loss: 0.3207\n",
            "Epoch 77/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3555\n",
            "Epoch 78/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8551 - loss: 0.3297\n",
            "Epoch 79/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8899 - loss: 0.3147\n",
            "Epoch 80/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8847 - loss: 0.3424\n",
            "Epoch 81/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8646 - loss: 0.3486\n",
            "Epoch 82/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8509 - loss: 0.3746\n",
            "Epoch 83/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8779 - loss: 0.3193\n",
            "Epoch 84/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8709 - loss: 0.3325\n",
            "Epoch 85/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8922 - loss: 0.3347\n",
            "Epoch 86/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8769 - loss: 0.3129\n",
            "Epoch 87/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8918 - loss: 0.3050\n",
            "Epoch 88/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8875 - loss: 0.3143\n",
            "Epoch 89/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8799 - loss: 0.3547\n",
            "Epoch 90/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8873 - loss: 0.3214\n",
            "Epoch 91/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 0.3449\n",
            "Epoch 92/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.3214\n",
            "Epoch 93/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8822 - loss: 0.3208\n",
            "Epoch 94/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8914 - loss: 0.3021\n",
            "Epoch 95/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8698 - loss: 0.3486\n",
            "Epoch 96/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.3055\n",
            "Epoch 97/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8841 - loss: 0.3119\n",
            "Epoch 98/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.3239\n",
            "Epoch 99/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8803 - loss: 0.3434\n",
            "Epoch 100/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8797 - loss: 0.3451\n",
            "Epoch 101/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8707 - loss: 0.3165\n",
            "Epoch 102/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8701 - loss: 0.3262\n",
            "Epoch 103/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.3165\n",
            "Epoch 104/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8843 - loss: 0.2910\n",
            "Epoch 105/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8646 - loss: 0.3484\n",
            "Epoch 106/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8677 - loss: 0.3266\n",
            "Epoch 107/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.3150\n",
            "Epoch 108/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8692 - loss: 0.3173\n",
            "Epoch 109/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8842 - loss: 0.2902\n",
            "Epoch 110/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8848 - loss: 0.3073\n",
            "Epoch 111/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8892 - loss: 0.3034\n",
            "Epoch 112/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.2966\n",
            "Epoch 113/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8825 - loss: 0.3056\n",
            "Epoch 114/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8778 - loss: 0.3070\n",
            "Epoch 115/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8632 - loss: 0.3218\n",
            "Epoch 116/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8884 - loss: 0.3298\n",
            "Epoch 117/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8759 - loss: 0.3180\n",
            "Epoch 118/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8630 - loss: 0.3222\n",
            "Epoch 119/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8738 - loss: 0.3153\n",
            "Epoch 120/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.3036\n",
            "Epoch 121/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8805 - loss: 0.3338\n",
            "Epoch 122/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8717 - loss: 0.3085\n",
            "Epoch 123/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.3033\n",
            "Epoch 124/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8941 - loss: 0.2924\n",
            "Epoch 125/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.2993\n",
            "Epoch 126/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.2862\n",
            "Epoch 127/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8576 - loss: 0.3480\n",
            "Epoch 128/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.3131\n",
            "Epoch 129/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8716 - loss: 0.2945\n",
            "Epoch 130/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8708 - loss: 0.3455\n",
            "Epoch 131/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3325\n",
            "Epoch 132/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8838 - loss: 0.3088\n",
            "Epoch 133/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8942 - loss: 0.2956\n",
            "Epoch 134/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8814 - loss: 0.3135\n",
            "Epoch 135/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.3259\n",
            "Epoch 136/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3397\n",
            "Epoch 137/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8925 - loss: 0.3148\n",
            "Epoch 138/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8890 - loss: 0.2871\n",
            "Epoch 139/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8834 - loss: 0.3255\n",
            "Epoch 140/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8836 - loss: 0.3055\n",
            "Epoch 141/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8925 - loss: 0.2892\n",
            "Epoch 142/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.3074\n",
            "Epoch 143/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8742 - loss: 0.3426\n",
            "Epoch 144/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3227\n",
            "Epoch 145/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8829 - loss: 0.3322\n",
            "Epoch 146/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8890 - loss: 0.2919\n",
            "Epoch 147/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8856 - loss: 0.3286\n",
            "Epoch 148/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8904 - loss: 0.3108\n",
            "Epoch 149/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3236\n",
            "Epoch 150/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.3005\n",
            "Epoch 151/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8786 - loss: 0.3167\n",
            "Epoch 152/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8897 - loss: 0.2979\n",
            "Epoch 153/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8972 - loss: 0.2958\n",
            "Epoch 154/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.3196\n",
            "Epoch 155/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8669 - loss: 0.3191\n",
            "Epoch 156/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8809 - loss: 0.3246\n",
            "Epoch 157/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8880 - loss: 0.2899\n",
            "Epoch 158/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8762 - loss: 0.2913\n",
            "Epoch 159/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8890 - loss: 0.3290\n",
            "Epoch 160/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8864 - loss: 0.3097\n",
            "Epoch 161/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8882 - loss: 0.3345\n",
            "Epoch 162/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8804 - loss: 0.2921\n",
            "Epoch 163/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8597 - loss: 0.3546\n",
            "Epoch 164/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8737 - loss: 0.3427\n",
            "Epoch 165/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8786 - loss: 0.2916\n",
            "Epoch 166/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8910 - loss: 0.2679\n",
            "Epoch 167/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8892 - loss: 0.3159\n",
            "Epoch 168/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8907 - loss: 0.2908\n",
            "Epoch 169/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8672 - loss: 0.3561\n",
            "Epoch 170/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8737 - loss: 0.2872\n",
            "Epoch 171/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8858 - loss: 0.2922\n",
            "Epoch 172/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.3132\n",
            "Epoch 173/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8843 - loss: 0.2876\n",
            "Epoch 174/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8759 - loss: 0.2918\n",
            "Epoch 175/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8835 - loss: 0.3659\n",
            "Epoch 176/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8758 - loss: 0.3278\n",
            "Epoch 177/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8575 - loss: 0.3162\n",
            "Epoch 178/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8965 - loss: 0.3129\n",
            "Epoch 179/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8894 - loss: 0.2766\n",
            "Epoch 180/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.2913\n",
            "Epoch 181/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8701 - loss: 0.3125\n",
            "Epoch 182/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8810 - loss: 0.3122\n",
            "Epoch 183/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8920 - loss: 0.2898\n",
            "Epoch 184/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8599 - loss: 0.3237\n",
            "Epoch 185/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.3213\n",
            "Epoch 186/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8915 - loss: 0.2926\n",
            "Epoch 187/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.3122\n",
            "Epoch 188/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8782 - loss: 0.2955\n",
            "Epoch 189/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8859 - loss: 0.2966\n",
            "Epoch 190/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 0.3076\n",
            "Epoch 191/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8849 - loss: 0.2999\n",
            "Epoch 192/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.3013\n",
            "Epoch 193/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8932 - loss: 0.2899\n",
            "Epoch 194/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8915 - loss: 0.2911\n",
            "Epoch 195/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8848 - loss: 0.3136\n",
            "Epoch 196/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8864 - loss: 0.2967\n",
            "Epoch 197/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8993 - loss: 0.2771\n",
            "Epoch 198/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8787 - loss: 0.3459\n",
            "Epoch 199/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8759 - loss: 0.2643\n",
            "Epoch 200/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3197\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91        75\n",
            "           1       0.84      0.68      0.75        31\n",
            "\n",
            "    accuracy                           0.87       106\n",
            "   macro avg       0.86      0.81      0.83       106\n",
            "weighted avg       0.87      0.87      0.86       106\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8623655913978495\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[71  4]\n",
            " [10 21]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8679245283018868\n",
            "\n",
            "37.275\n",
            "------------------->>>>>>>>>>Fold no =  5\n",
            "Epoch 1/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5948 - loss: 0.6729\n",
            "Epoch 2/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6854 - loss: 0.6512\n",
            "Epoch 3/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6890 - loss: 0.6478\n",
            "Epoch 4/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6853 - loss: 0.6160\n",
            "Epoch 5/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6818 - loss: 0.5876\n",
            "Epoch 6/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7170 - loss: 0.5960\n",
            "Epoch 7/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7346 - loss: 0.5826\n",
            "Epoch 8/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7412 - loss: 0.5716\n",
            "Epoch 9/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7412 - loss: 0.5508\n",
            "Epoch 10/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.5447\n",
            "Epoch 11/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7974 - loss: 0.5189\n",
            "Epoch 12/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7796 - loss: 0.5391\n",
            "Epoch 13/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7755 - loss: 0.5170\n",
            "Epoch 14/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7591 - loss: 0.5060\n",
            "Epoch 15/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7896 - loss: 0.5134\n",
            "Epoch 16/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.5178\n",
            "Epoch 17/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8242 - loss: 0.4903\n",
            "Epoch 18/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.5018\n",
            "Epoch 19/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8038 - loss: 0.4818\n",
            "Epoch 20/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8111 - loss: 0.4931\n",
            "Epoch 21/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8083 - loss: 0.4420\n",
            "Epoch 22/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7899 - loss: 0.4934\n",
            "Epoch 23/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8129 - loss: 0.4634\n",
            "Epoch 24/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8113 - loss: 0.4776\n",
            "Epoch 25/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7858 - loss: 0.4868\n",
            "Epoch 26/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8166 - loss: 0.4545\n",
            "Epoch 27/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.4582\n",
            "Epoch 28/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8236 - loss: 0.4555\n",
            "Epoch 29/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8386 - loss: 0.4206\n",
            "Epoch 30/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8184 - loss: 0.4606\n",
            "Epoch 31/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8267 - loss: 0.4757\n",
            "Epoch 32/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8156 - loss: 0.4822\n",
            "Epoch 33/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.4128\n",
            "Epoch 34/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.4711\n",
            "Epoch 35/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8273 - loss: 0.4526\n",
            "Epoch 36/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.4190\n",
            "Epoch 37/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8328 - loss: 0.4180\n",
            "Epoch 38/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8508 - loss: 0.4308\n",
            "Epoch 39/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.4084\n",
            "Epoch 40/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8332 - loss: 0.4112\n",
            "Epoch 41/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8476 - loss: 0.4174\n",
            "Epoch 42/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.4066\n",
            "Epoch 43/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8606 - loss: 0.4297\n",
            "Epoch 44/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8331 - loss: 0.4531\n",
            "Epoch 45/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8523 - loss: 0.4034\n",
            "Epoch 46/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8543 - loss: 0.4473\n",
            "Epoch 47/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8414 - loss: 0.4305\n",
            "Epoch 48/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8618 - loss: 0.4020\n",
            "Epoch 49/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8647 - loss: 0.4035\n",
            "Epoch 50/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8380 - loss: 0.4547\n",
            "Epoch 51/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8302 - loss: 0.4017\n",
            "Epoch 52/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8476 - loss: 0.4117\n",
            "Epoch 53/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8518 - loss: 0.4147\n",
            "Epoch 54/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8382 - loss: 0.4056\n",
            "Epoch 55/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.4282\n",
            "Epoch 56/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8318 - loss: 0.4019\n",
            "Epoch 57/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8358 - loss: 0.4189\n",
            "Epoch 58/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8604 - loss: 0.3691\n",
            "Epoch 59/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8504 - loss: 0.4319\n",
            "Epoch 60/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8488 - loss: 0.4070\n",
            "Epoch 61/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4250\n",
            "Epoch 62/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8538 - loss: 0.4030\n",
            "Epoch 63/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8555 - loss: 0.3973\n",
            "Epoch 64/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.4375\n",
            "Epoch 65/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8317 - loss: 0.4323\n",
            "Epoch 66/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8512 - loss: 0.4158\n",
            "Epoch 67/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8351 - loss: 0.4416\n",
            "Epoch 68/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.4201\n",
            "Epoch 69/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8335 - loss: 0.4151\n",
            "Epoch 70/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8250 - loss: 0.4252\n",
            "Epoch 71/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8334 - loss: 0.4178\n",
            "Epoch 72/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8479 - loss: 0.3878\n",
            "Epoch 73/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4079\n",
            "Epoch 74/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4250\n",
            "Epoch 75/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4352\n",
            "Epoch 76/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8493 - loss: 0.4309\n",
            "Epoch 77/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8610 - loss: 0.3792\n",
            "Epoch 78/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8556 - loss: 0.4393\n",
            "Epoch 79/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8480 - loss: 0.3985\n",
            "Epoch 80/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8667 - loss: 0.3984\n",
            "Epoch 81/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8457 - loss: 0.3994\n",
            "Epoch 82/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.4312\n",
            "Epoch 83/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8361 - loss: 0.4113\n",
            "Epoch 84/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8407 - loss: 0.4075\n",
            "Epoch 85/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8516 - loss: 0.4010\n",
            "Epoch 86/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8456 - loss: 0.4115\n",
            "Epoch 87/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8350 - loss: 0.4373\n",
            "Epoch 88/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8372 - loss: 0.4224\n",
            "Epoch 89/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.4449\n",
            "Epoch 90/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8407 - loss: 0.4383\n",
            "Epoch 91/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8605 - loss: 0.3798\n",
            "Epoch 92/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8609 - loss: 0.4216\n",
            "Epoch 93/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8470 - loss: 0.3659\n",
            "Epoch 94/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.4011\n",
            "Epoch 95/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8445 - loss: 0.3945\n",
            "Epoch 96/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8531 - loss: 0.4219\n",
            "Epoch 97/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8258 - loss: 0.3700\n",
            "Epoch 98/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8519 - loss: 0.3724\n",
            "Epoch 99/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8546 - loss: 0.4006\n",
            "Epoch 100/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8210 - loss: 0.4445\n",
            "Epoch 101/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8455 - loss: 0.3856\n",
            "Epoch 102/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8469 - loss: 0.3868\n",
            "Epoch 103/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8332 - loss: 0.3999\n",
            "Epoch 104/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8492 - loss: 0.3722\n",
            "Epoch 105/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8397 - loss: 0.3846\n",
            "Epoch 106/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8492 - loss: 0.4028\n",
            "Epoch 107/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8544 - loss: 0.3758\n",
            "Epoch 108/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8669 - loss: 0.3720\n",
            "Epoch 109/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8510 - loss: 0.3734\n",
            "Epoch 110/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8391 - loss: 0.3868\n",
            "Epoch 111/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8605 - loss: 0.4126\n",
            "Epoch 112/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8431 - loss: 0.4003\n",
            "Epoch 113/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8284 - loss: 0.4140\n",
            "Epoch 114/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.3784\n",
            "Epoch 115/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8473 - loss: 0.3762\n",
            "Epoch 116/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8424 - loss: 0.3952\n",
            "Epoch 117/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8513 - loss: 0.3590\n",
            "Epoch 118/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.4386\n",
            "Epoch 119/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8466 - loss: 0.3752\n",
            "Epoch 120/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.4019\n",
            "Epoch 121/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8370 - loss: 0.3819\n",
            "Epoch 122/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8472 - loss: 0.3995\n",
            "Epoch 123/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8342 - loss: 0.3771\n",
            "Epoch 124/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.3520\n",
            "Epoch 125/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8572 - loss: 0.3656\n",
            "Epoch 126/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8464 - loss: 0.4159\n",
            "Epoch 127/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8330 - loss: 0.3517\n",
            "Epoch 128/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8553 - loss: 0.3667\n",
            "Epoch 129/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.3978\n",
            "Epoch 130/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8449 - loss: 0.3821\n",
            "Epoch 131/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.3785\n",
            "Epoch 132/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8242 - loss: 0.3872\n",
            "Epoch 133/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8346 - loss: 0.3628\n",
            "Epoch 134/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8417 - loss: 0.3950\n",
            "Epoch 135/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8389 - loss: 0.3999\n",
            "Epoch 136/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.4036\n",
            "Epoch 137/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8517 - loss: 0.3815\n",
            "Epoch 138/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8424 - loss: 0.4029\n",
            "Epoch 139/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.3794\n",
            "Epoch 140/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8439 - loss: 0.3951\n",
            "Epoch 141/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8259 - loss: 0.4023\n",
            "Epoch 142/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8426 - loss: 0.4051\n",
            "Epoch 143/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8443 - loss: 0.3509\n",
            "Epoch 144/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8245 - loss: 0.4177\n",
            "Epoch 145/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8412 - loss: 0.3663\n",
            "Epoch 146/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.3963\n",
            "Epoch 147/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8540 - loss: 0.3637\n",
            "Epoch 148/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8325 - loss: 0.4054\n",
            "Epoch 149/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8495 - loss: 0.3859\n",
            "Epoch 150/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8409 - loss: 0.3920\n",
            "Epoch 151/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8434 - loss: 0.3604\n",
            "Epoch 152/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.4083\n",
            "Epoch 153/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8449 - loss: 0.4012\n",
            "Epoch 154/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8501 - loss: 0.3905\n",
            "Epoch 155/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.3925\n",
            "Epoch 156/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8479 - loss: 0.3958\n",
            "Epoch 157/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.4004\n",
            "Epoch 158/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3485\n",
            "Epoch 159/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8162 - loss: 0.4088\n",
            "Epoch 160/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8483 - loss: 0.3755\n",
            "Epoch 161/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8625 - loss: 0.3842\n",
            "Epoch 162/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8503 - loss: 0.3992\n",
            "Epoch 163/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8624 - loss: 0.3894\n",
            "Epoch 164/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8720 - loss: 0.3385\n",
            "Epoch 165/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8259 - loss: 0.4234\n",
            "Epoch 166/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8356 - loss: 0.4011\n",
            "Epoch 167/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8314 - loss: 0.4496\n",
            "Epoch 168/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8587 - loss: 0.3781\n",
            "Epoch 169/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8500 - loss: 0.3749\n",
            "Epoch 170/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8340 - loss: 0.3738\n",
            "Epoch 171/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8468 - loss: 0.3670\n",
            "Epoch 172/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8572 - loss: 0.4112\n",
            "Epoch 173/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8580 - loss: 0.3973\n",
            "Epoch 174/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8613 - loss: 0.3618\n",
            "Epoch 175/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8540 - loss: 0.3743\n",
            "Epoch 176/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8661 - loss: 0.4054\n",
            "Epoch 177/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8473 - loss: 0.3708\n",
            "Epoch 178/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8465 - loss: 0.3508\n",
            "Epoch 179/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.3674\n",
            "Epoch 180/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8661 - loss: 0.3778\n",
            "Epoch 181/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8720 - loss: 0.3645\n",
            "Epoch 182/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8465 - loss: 0.3892\n",
            "Epoch 183/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8580 - loss: 0.3602\n",
            "Epoch 184/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 0.3810\n",
            "Epoch 185/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8517 - loss: 0.3987\n",
            "Epoch 186/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8570 - loss: 0.3822\n",
            "Epoch 187/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8525 - loss: 0.3834\n",
            "Epoch 188/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8671 - loss: 0.3878\n",
            "Epoch 189/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8474 - loss: 0.4087\n",
            "Epoch 190/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8554 - loss: 0.3928\n",
            "Epoch 191/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8534 - loss: 0.4097\n",
            "Epoch 192/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8681 - loss: 0.3837\n",
            "Epoch 193/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8572 - loss: 0.3780\n",
            "Epoch 194/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8379 - loss: 0.3883\n",
            "Epoch 195/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8540 - loss: 0.3787\n",
            "Epoch 196/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8489 - loss: 0.3627\n",
            "Epoch 197/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.4116\n",
            "Epoch 198/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8422 - loss: 0.4130\n",
            "Epoch 199/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8608 - loss: 0.3687\n",
            "Epoch 200/200\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8512 - loss: 0.3519\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94        75\n",
            "           1       0.92      0.74      0.82        31\n",
            "\n",
            "    accuracy                           0.91       106\n",
            "   macro avg       0.91      0.86      0.88       106\n",
            "weighted avg       0.91      0.91      0.90       106\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.941505376344086\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[73  2]\n",
            " [ 8 23]]\n",
            "\n",
            "Accuracy for Current Fold: 0.9056603773584906\n",
            "\n",
            "104.9375\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Average---------------------\n",
            "AUC (Avg. +/- Std.) is  0.901 +/- 0.030\n",
            "Accuracy (Avg. +/- Std.) is  0.862 +/- 0.030\n",
            "Avg. CM is [[22, 6], [8, 68]]\n",
            "Total for all folds CM is [[113, 31], [42, 344]]\n",
            "Sensitivity (Avg. +/- Std.) is  0.729 +/- 0.048\n",
            "Specificity (Avg. +/- Std.) is  0.917 +/- 0.038\n",
            "Precision (Avg. +/- Std.) is  0.792 +/- 0.082\n",
            "FOR (Avg. +/- Std.) is  0.109 +/- 0.018\n",
            "DOR (Avg. +/- Std.) is  43.737 +/- 31.688\n"
          ]
        }
      ],
      "source": [
        "i=0                                                                          # for verification of fold number\n",
        "Accuracy = []                                                                # for store the value of accuracy\n",
        "FP = []                                                                      # for store False Positive\n",
        "TN = []                                                                      # for True Negative\n",
        "FN = []                                                                      # for False Negative\n",
        "TP = []                                                                      # for True Positive\n",
        "tprs = []                                                                    # for true positive rates\n",
        "aucs_ens = []                                                                # for store the values of auc\n",
        "sn = []                                                                      # for sensitivity\n",
        "sp = []                                                                      # for specificity\n",
        "pr = []                                                                      # for precision\n",
        "FOR = []                                                                     # for False omission rate\n",
        "DOR = []\n",
        "iterator=0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "#best optimizers found  are defined here by experiment\n",
        "activation=\"relu\"\n",
        "batch_size=8\n",
        "epochs=200\n",
        "learn_rate=.001\n",
        "dropout_rate=0.6\n",
        "init=\"normal\"\n",
        "neuron1,neuron2,neuron3,neuron4=64,16,64,64\n",
        "print(activation,batch_size,epochs,learn_rate,dropout_rate,init,\n",
        "    neuron1,neuron2,neuron3,neuron4)\n",
        "\n",
        "\n",
        "\n",
        "for train_index, test_index in kf.split(X_Data,Y_Lavel):                  # for k fold experiment\n",
        "  print('------------------->>>>>>>>>>Fold no = ',i+1)\n",
        "  X_Train, X_Test = X_Data[train_index], X_Data[test_index]               # the train data and label\n",
        "  Y_Train, Y_Test = Y_Lavel[train_index], Y_Lavel[test_index]             # the  test data and label\n",
        "\n",
        "\n",
        "  Y_Train_1Hot = to_categorical(Y_Train,2)                                #convert train output to catagorical\n",
        "  Y_Test_1Hot = to_categorical(Y_Test,2)                                  #convert test output to catagorical\n",
        "\n",
        "  model =nn_opt(activation,                                               #build model using tuned parameters\n",
        "            dropout_rate,\n",
        "            init,\n",
        "            learn_rate)\n",
        "  np.random.seed(6)\n",
        "  model.fit(x=X_Train,                                                     # fit our model\n",
        "            y=Y_Train_1Hot,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            shuffle=False,\n",
        "            verbose=1)\n",
        "\n",
        "  probas_ = model.predict(X_Test)                                           #predict the class probability\n",
        "\n",
        "  y_pred = np.argmax(model.predict(X_Test), axis=1)                         #find the max. probability of output class\n",
        "\n",
        "\n",
        "  tn, fp, fn, tp, roc_auc, fpr, tpr = metrics (y_true = Y_Test,           #evaluation parameters of ensembelled model\n",
        "                                              y_pred = y_pred,\n",
        "                                              probas_ = probas_)\n",
        "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "  tprs[-1][0] = 0.0\n",
        "  aucs_ens.append(roc_auc)\n",
        "  plot_Current_ROC (fpr,tpr,iterator,roc_auc)                             #plot the ROC curve of current fold\n",
        "  iterator += 1\n",
        "  TN.append(tn)\n",
        "  FP.append(fp)\n",
        "  FN.append(fn)\n",
        "  TP.append(tp)\n",
        "  Accuracy.append(accuracy_score(Y_Test, y_pred))\n",
        "  sn.append(tp/(tp+fn))\n",
        "  sp.append(tn/(fp+tn))\n",
        "  pr.append(tp/(tp+fp))\n",
        "  FOR.append(fn/(tn+fn))\n",
        "  DOR.append((tp*tn)/(fp*fn))\n",
        "  print((tp*tn)/(fp*fn))\n",
        "  i+=1\n",
        "\n",
        "average_ROC(mean_fpr,tprs,aucs_ens,TP,TN,FP,FN)                             #plot average ROC\n",
        "average_performance(aucs_ens,Accuracy,TP,TN,FP,FN)                          #print the average performance\n",
        "print(\"Sensitivity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sn),np.std(sn)))\n",
        "print(\"Specificity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sp),np.std(sp)))\n",
        "print(\"Precision (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(pr),np.std(pr)))\n",
        "print(\"FOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(FOR),np.std(FOR)))\n",
        "print(\"DOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(DOR),np.std(DOR)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XR7VYoBaUZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}